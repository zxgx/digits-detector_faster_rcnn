{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"faster rcnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["wqkfPLfEqZu8","pWqJjRQQxr17","VIk9DxVxxpMj","o1TBf3zZyw0T","hCbrtq5y0yEa","4lKQyzZk1U1p","Df26pH221DlS","KfwC8o4i0fSc","SvmctCan2T3r","_mf4J6Hp0ora","xAnPRfHc3_OJ","E2fc8QBZ145x","fXVW39bC0GOi","MvyBhgCvSZTN","quEPH82HVl--","OEDfian4Yzyx"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2m--dMEjg8U3","colab_type":"code","outputId":"8644ceff-57d4-4e16-a677-6a7fce5e75d5","executionInfo":{"status":"ok","timestamp":1559023275454,"user_tz":-480,"elapsed":1233,"user":{"displayName":"张耿","photoUrl":"","userId":"17787706570393703878"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","#\"gdrive/My Drive/svhn/\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wqkfPLfEqZu8","colab_type":"text"},"source":["# Config"]},{"cell_type":"code","metadata":{"id":"XdF3f8WbqLkX","colab_type":"code","colab":{}},"source":["from pprint import pprint\n","\n","\n","# Default Configs for training\n","# NOTE that, config items could be overwriten by passing argument through command line.\n","# e.g. --voc-data-dir='./data/'\n","\n","class Config:\n","    # data\n","    voc_data_dir = 'gdrive/My Drive/svhn/data/VOCdevkit/VOC2007/'\n","    min_size = 600  # image resize\n","    max_size = 1000 # image resize\n","    num_workers = 8\n","    test_num_workers = 8\n","\n","    # sigma for l1_smooth_loss\n","    rpn_sigma = 3.\n","    roi_sigma = 1.\n","\n","    # param for optimizer\n","    # 0.0005 in origin paper but 0.0001 in tf-faster-rcnn\n","    weight_decay = 0.0005\n","    lr_decay = 0.1  # 1e-3 -> 1e-4\n","    lr = 1e-3\n","\n","\n","    # visualization\n","    env = 'faster-rcnn'  # visdom env\n","    port = 8097\n","    plot_every = 40  # vis every N iter\n","\n","    # preset\n","    data = 'voc'\n","    pretrained_model = 'vgg16'\n","\n","    # training\n","    epoch = 14\n","\n","\n","    use_adam = False # Use Adam optimizer\n","    use_chainer = False # try match everything as chainer\n","    use_drop = False # use dropout in RoIHead\n","    # debug\n","    debug_file = '/tmp/debugf'\n","\n","    test_num = 10000\n","    # model\n","    load_path = None\n","\n","    caffe_pretrain = False # use caffe pretrained model instead of torchvision\n","    caffe_pretrain_path = 'checkpoints/vgg16_caffe.pth'\n","\n","    def _parse(self, kwargs):\n","        state_dict = self._state_dict()\n","        for k, v in kwargs.items():\n","            if k not in state_dict:\n","                raise ValueError('UnKnown Option: \"--%s\"' % k)\n","            setattr(self, k, v)\n","\n","        print('======user config========')\n","        pprint(self._state_dict())\n","        print('==========end============')\n","\n","    def _state_dict(self):\n","        return {k: getattr(self, k) for k, _ in Config.__dict__.items() \\\n","                if not k.startswith('_')}\n","\n","\n","opt = Config()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SnWCgeWkqsMh","colab_type":"text"},"source":["# Dataset"]},{"cell_type":"markdown","metadata":{"id":"pWqJjRQQxr17","colab_type":"text"},"source":["## data.util"]},{"cell_type":"code","metadata":{"id":"b_ykmGgwx7Mr","colab_type":"code","colab":{}},"source":["import numpy as np\n","from PIL import Image\n","import random\n","\n","\n","def read_image(path, dtype=np.float32, color=True):\n","    \"\"\"Read an image from a file.\n","\n","    This function reads an image from given file. The image is CHW format and\n","    the range of its value is :math:`[0, 255]`. If :obj:`color = True`, the\n","    order of the channels is RGB.\n","\n","    Args:\n","        path (str): A path of image file.\n","        dtype: The type of array. The default value is :obj:`~numpy.float32`.\n","        color (bool): This option determines the number of channels.\n","            If :obj:`True`, the number of channels is three. In this case,\n","            the order of the channels is RGB. This is the default behaviour.\n","            If :obj:`False`, this function returns a grayscale image.\n","\n","    Returns:\n","        ~numpy.ndarray: An image.\n","    \"\"\"\n","\n","    f = Image.open(path)\n","    try:\n","        if color:\n","            img = f.convert('RGB')\n","        else:\n","            img = f.convert('P')\n","        img = np.asarray(img, dtype=dtype)\n","    finally:\n","        if hasattr(f, 'close'):\n","            f.close()\n","\n","    if img.ndim == 2:\n","        # reshape (H, W) -> (1, H, W)\n","        return img[np.newaxis]\n","    else:\n","        # transpose (H, W, C) -> (C, H, W)\n","        return img.transpose((2, 0, 1))\n","\n","\n","def resize_bbox(bbox, in_size, out_size):\n","    \"\"\"Resize bounding boxes according to image resize.\n","\n","    The bounding boxes are expected to be packed into a two dimensional\n","    tensor of shape :math:`(R, 4)`, where :math:`R` is the number of\n","    bounding boxes in the image. The second axis represents attributes of\n","    the bounding box. They are :math:`(y_{min}, x_{min}, y_{max}, x_{max})`,\n","    where the four attributes are coordinates of the top left and the\n","    bottom right vertices.\n","\n","    Args:\n","        bbox (~numpy.ndarray): An array whose shape is :math:`(R, 4)`.\n","            :math:`R` is the number of bounding boxes.\n","        in_size (tuple): A tuple of length 2. The height and the width\n","            of the image before resized.\n","        out_size (tuple): A tuple of length 2. The height and the width\n","            of the image after resized.\n","\n","    Returns:\n","        ~numpy.ndarray:\n","        Bounding boxes rescaled according to the given image shapes.\n","\n","    \"\"\"\n","    bbox = bbox.copy()\n","    y_scale = float(out_size[0]) / in_size[0]\n","    x_scale = float(out_size[1]) / in_size[1]\n","    bbox[:, 0] = y_scale * bbox[:, 0]\n","    bbox[:, 2] = y_scale * bbox[:, 2]\n","    bbox[:, 1] = x_scale * bbox[:, 1]\n","    bbox[:, 3] = x_scale * bbox[:, 3]\n","    return bbox\n","\n","\n","def flip_bbox(bbox, size, y_flip=False, x_flip=False):\n","    \"\"\"Flip bounding boxes accordingly.\n","\n","    The bounding boxes are expected to be packed into a two dimensional\n","    tensor of shape :math:`(R, 4)`, where :math:`R` is the number of\n","    bounding boxes in the image. The second axis represents attributes of\n","    the bounding box. They are :math:`(y_{min}, x_{min}, y_{max}, x_{max})`,\n","    where the four attributes are coordinates of the top left and the\n","    bottom right vertices.\n","\n","    Args:\n","        bbox (~numpy.ndarray): An array whose shape is :math:`(R, 4)`.\n","            :math:`R` is the number of bounding boxes.\n","        size (tuple): A tuple of length 2. The height and the width\n","            of the image before resized.\n","        y_flip (bool): Flip bounding box according to a vertical flip of\n","            an image.\n","        x_flip (bool): Flip bounding box according to a horizontal flip of\n","            an image.\n","\n","    Returns:\n","        ~numpy.ndarray:\n","        Bounding boxes flipped according to the given flips.\n","\n","    \"\"\"\n","    H, W = size\n","    bbox = bbox.copy()\n","    if y_flip:\n","        y_max = H - bbox[:, 0]\n","        y_min = H - bbox[:, 2]\n","        bbox[:, 0] = y_min\n","        bbox[:, 2] = y_max\n","    if x_flip:\n","        x_max = W - bbox[:, 1]\n","        x_min = W - bbox[:, 3]\n","        bbox[:, 1] = x_min\n","        bbox[:, 3] = x_max\n","    return bbox\n","\n","\n","def crop_bbox(\n","        bbox, y_slice=None, x_slice=None,\n","        allow_outside_center=True, return_param=False):\n","    \"\"\"Translate bounding boxes to fit within the cropped area of an image.\n","\n","    This method is mainly used together with image cropping.\n","    This method translates the coordinates of bounding boxes like\n","    :func:`data.util.translate_bbox`. In addition,\n","    this function truncates the bounding boxes to fit within the cropped area.\n","    If a bounding box does not overlap with the cropped area,\n","    this bounding box will be removed.\n","\n","    The bounding boxes are expected to be packed into a two dimensional\n","    tensor of shape :math:`(R, 4)`, where :math:`R` is the number of\n","    bounding boxes in the image. The second axis represents attributes of\n","    the bounding box. They are :math:`(y_{min}, x_{min}, y_{max}, x_{max})`,\n","    where the four attributes are coordinates of the top left and the\n","    bottom right vertices.\n","\n","    Args:\n","        bbox (~numpy.ndarray): Bounding boxes to be transformed. The shape is\n","            :math:`(R, 4)`. :math:`R` is the number of bounding boxes.\n","        y_slice (slice): The slice of y axis.\n","        x_slice (slice): The slice of x axis.\n","        allow_outside_center (bool): If this argument is :obj:`False`,\n","            bounding boxes whose centers are outside of the cropped area\n","            are removed. The default value is :obj:`True`.\n","        return_param (bool): If :obj:`True`, this function returns\n","            indices of kept bounding boxes.\n","\n","    Returns:\n","        ~numpy.ndarray or (~numpy.ndarray, dict):\n","\n","        If :obj:`return_param = False`, returns an array :obj:`bbox`.\n","\n","        If :obj:`return_param = True`,\n","        returns a tuple whose elements are :obj:`bbox, param`.\n","        :obj:`param` is a dictionary of intermediate parameters whose\n","        contents are listed below with key, value-type and the description\n","        of the value.\n","\n","        * **index** (*numpy.ndarray*): An array holding indices of used \\\n","            bounding boxes.\n","\n","    \"\"\"\n","\n","    t, b = _slice_to_bounds(y_slice)\n","    l, r = _slice_to_bounds(x_slice)\n","    crop_bb = np.array((t, l, b, r))\n","\n","    if allow_outside_center:\n","        mask = np.ones(bbox.shape[0], dtype=bool)\n","    else:\n","        center = (bbox[:, :2] + bbox[:, 2:]) / 2.0\n","        mask = np.logical_and(crop_bb[:2] <= center, center < crop_bb[2:]) \\\n","            .all(axis=1)\n","\n","    bbox = bbox.copy()\n","    bbox[:, :2] = np.maximum(bbox[:, :2], crop_bb[:2])\n","    bbox[:, 2:] = np.minimum(bbox[:, 2:], crop_bb[2:])\n","    bbox[:, :2] -= crop_bb[:2]\n","    bbox[:, 2:] -= crop_bb[:2]\n","\n","    mask = np.logical_and(mask, (bbox[:, :2] < bbox[:, 2:]).all(axis=1))\n","    bbox = bbox[mask]\n","\n","    if return_param:\n","        return bbox, {'index': np.flatnonzero(mask)}\n","    else:\n","        return bbox\n","\n","\n","def _slice_to_bounds(slice_):\n","    if slice_ is None:\n","        return 0, np.inf\n","\n","    if slice_.start is None:\n","        l = 0\n","    else:\n","        l = slice_.start\n","\n","    if slice_.stop is None:\n","        u = np.inf\n","    else:\n","        u = slice_.stop\n","\n","    return l, u\n","\n","\n","def translate_bbox(bbox, y_offset=0, x_offset=0):\n","    \"\"\"Translate bounding boxes.\n","\n","    This method is mainly used together with image transforms, such as padding\n","    and cropping, which translates the left top point of the image from\n","    coordinate :math:`(0, 0)` to coordinate\n","    :math:`(y, x) = (y_{offset}, x_{offset})`.\n","\n","    The bounding boxes are expected to be packed into a two dimensional\n","    tensor of shape :math:`(R, 4)`, where :math:`R` is the number of\n","    bounding boxes in the image. The second axis represents attributes of\n","    the bounding box. They are :math:`(y_{min}, x_{min}, y_{max}, x_{max})`,\n","    where the four attributes are coordinates of the top left and the\n","    bottom right vertices.\n","\n","    Args:\n","        bbox (~numpy.ndarray): Bounding boxes to be transformed. The shape is\n","            :math:`(R, 4)`. :math:`R` is the number of bounding boxes.\n","        y_offset (int or float): The offset along y axis.\n","        x_offset (int or float): The offset along x axis.\n","\n","    Returns:\n","        ~numpy.ndarray:\n","        Bounding boxes translated according to the given offsets.\n","\n","    \"\"\"\n","\n","    out_bbox = bbox.copy()\n","    out_bbox[:, :2] += (y_offset, x_offset)\n","    out_bbox[:, 2:] += (y_offset, x_offset)\n","\n","    return out_bbox\n","\n","\n","def random_flip(img, y_random=False, x_random=False,\n","                return_param=False, copy=False):\n","    \"\"\"Randomly flip an image in vertical or horizontal direction.\n","\n","    Args:\n","        img (~numpy.ndarray): An array that gets flipped. This is in\n","            CHW format.\n","        y_random (bool): Randomly flip in vertical direction.\n","        x_random (bool): Randomly flip in horizontal direction.\n","        return_param (bool): Returns information of flip.\n","        copy (bool): If False, a view of :obj:`img` will be returned.\n","\n","    Returns:\n","        ~numpy.ndarray or (~numpy.ndarray, dict):\n","\n","        If :obj:`return_param = False`,\n","        returns an array :obj:`out_img` that is the result of flipping.\n","\n","        If :obj:`return_param = True`,\n","        returns a tuple whose elements are :obj:`out_img, param`.\n","        :obj:`param` is a dictionary of intermediate parameters whose\n","        contents are listed below with key, value-type and the description\n","        of the value.\n","\n","        * **y_flip** (*bool*): Whether the image was flipped in the\\\n","            vertical direction or not.\n","        * **x_flip** (*bool*): Whether the image was flipped in the\\\n","            horizontal direction or not.\n","\n","    \"\"\"\n","    y_flip, x_flip = False, False\n","    if y_random:\n","        y_flip = random.choice([True, False])\n","    if x_random:\n","        x_flip = random.choice([True, False])\n","\n","    if y_flip:\n","        img = img[:, ::-1, :]\n","    if x_flip:\n","        img = img[:, :, ::-1]\n","\n","    if copy:\n","        img = img.copy()\n","\n","    if return_param:\n","        return img, {'y_flip': y_flip, 'x_flip': x_flip}\n","    else:\n","        return img\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VIk9DxVxxpMj","colab_type":"text"},"source":["## data.voc_dataset"]},{"cell_type":"code","metadata":{"id":"2Y247QFLyEH7","colab_type":"code","colab":{}},"source":["import os\n","import xml.etree.ElementTree as ET\n","\n","import numpy as np\n","\n","#from .util import read_image\n","\n","\n","class VOCBboxDataset:\n","\n","    def __init__(self, data_dir, split='trainval',\n","                 use_difficult=False, return_difficult=False,\n","                 ):\n","        id_list_file = os.path.join(\n","            data_dir, 'ImageSets/Main/{0}.txt'.format(split))\n","\n","        self.ids = [id_.strip() for id_ in open(id_list_file)]\n","        self.data_dir = data_dir\n","        self.use_difficult = use_difficult\n","        self.return_difficult = return_difficult\n","        self.label_names = VOC_BBOX_LABEL_NAMES\n","\n","    def __len__(self):\n","        return len(self.ids)\n","\n","    def get_example(self, i):\n","        id_ = self.ids[i]\n","        anno = ET.parse(\n","            os.path.join(self.data_dir, 'Annotations', id_ + '.xml'))\n","        bbox = list()\n","        label = list()\n","        difficult = list()\n","        for obj in anno.findall('object'):\n","            # when in not using difficult split, and the object is\n","            # difficult, skipt it.\n","            if not self.use_difficult and int(obj.find('difficult').text) == 1:\n","                continue\n","\n","            difficult.append(int(obj.find('difficult').text))\n","            bndbox_anno = obj.find('bndbox')\n","            # subtract 1 to make pixel indexes 0-based\n","            bbox.append([\n","                int(bndbox_anno.find(tag).text) - 1\n","                for tag in ('ymin', 'xmin', 'ymax', 'xmax')])\n","            name = obj.find('name').text.lower().strip()\n","            label.append(VOC_BBOX_LABEL_NAMES.index(name))\n","        bbox = np.stack(bbox).astype(np.float32)\n","        label = np.stack(label).astype(np.int32)\n","        # When `use_difficult==False`, all elements in `difficult` are False.\n","        difficult = np.array(difficult, dtype=np.bool).astype(np.uint8)  # PyTorch don't support np.bool\n","\n","        # Load a image\n","        img_file = os.path.join(self.data_dir, 'JPEGImages', id_ + '.jpg')\n","        img = read_image(img_file, color=True)\n","\n","        # if self.return_difficult:\n","        #     return img, bbox, label, difficult\n","        return img, bbox, label, difficult\n","\n","    __getitem__ = get_example\n","\n","\n","VOC_BBOX_LABEL_NAMES = (\n","    'aeroplane',\n","    'bicycle',\n","    'bird',\n","    'boat',\n","    'bottle',\n","    'bus',\n","    'car',\n","    'cat',\n","    'chair',\n","    'cow',\n","    'diningtable',\n","    'dog',\n","    'horse',\n","    'motorbike',\n","    'person',\n","    'pottedplant',\n","    'sheep',\n","    'sofa',\n","    'train',\n","    'tvmonitor')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4hYhky4HxbIz","colab_type":"text"},"source":["## data.dataset"]},{"cell_type":"code","metadata":{"id":"dYCjirgDq0P3","colab_type":"code","colab":{}},"source":["import torch as t\n","#from data.voc_dataset import VOCBboxDataset\n","from skimage import transform as sktsf\n","from torchvision import transforms as tvtsf\n","#from data import util\n","import numpy as np\n","#from utils.config import opt\n","\n","\n","def inverse_normalize(img):\n","    if opt.caffe_pretrain:\n","        img = img + (np.array([122.7717, 115.9465, 102.9801]).reshape(3, 1, 1))\n","        return img[::-1, :, :]\n","    # approximate un-normalize for visualize\n","    return (img * 0.225 + 0.45).clip(min=0, max=1) * 255\n","\n","\n","def pytorch_normalze(img):\n","    \"\"\"\n","    https://github.com/pytorch/vision/issues/223\n","    return appr -1~1 RGB\n","    \"\"\"\n","    normalize = tvtsf.Normalize(mean=[0.485, 0.456, 0.406],\n","                                std=[0.229, 0.224, 0.225])\n","    img = normalize(t.from_numpy(img))\n","    return img.numpy()\n","\n","\n","def caffe_normalize(img):\n","    \"\"\"\n","    return appr -125-125 BGR\n","    \"\"\"\n","    img = img[[2, 1, 0], :, :]  # RGB-BGR\n","    img = img * 255\n","    mean = np.array([122.7717, 115.9465, 102.9801]).reshape(3, 1, 1)\n","    img = (img - mean).astype(np.float32, copy=True)\n","    return img\n","\n","\n","def preprocess(img, min_size=600, max_size=1000):\n","    C, H, W = img.shape\n","    scale1 = min_size / min(H, W)\n","    scale2 = max_size / max(H, W)\n","    scale = min(scale1, scale2)\n","    img = img / 255.\n","    img = sktsf.resize(img, (C, H * scale, W * scale), mode='reflect',anti_aliasing=False)\n","    # both the longer and shorter should be less than\n","    # max_size and min_size\n","    img = img.astype(np.float32) ###### added\n","    if opt.caffe_pretrain:\n","        normalize = caffe_normalize\n","    else:\n","        normalize = pytorch_normalze\n","    return normalize(img)\n","\n","\n","class Transform(object):\n","\n","    def __init__(self, min_size=600, max_size=1000):\n","        self.min_size = min_size\n","        self.max_size = max_size\n","\n","    def __call__(self, in_data):\n","        img, bbox, label = in_data\n","        _, H, W = img.shape\n","        img = preprocess(img, self.min_size, self.max_size)\n","        _, o_H, o_W = img.shape\n","        scale = o_H / H\n","        bbox = resize_bbox(bbox, (H, W), (o_H, o_W))\n","\n","        # horizontally flip\n","        img, params = random_flip(\n","            img, x_random=True, return_param=True)\n","        bbox = flip_bbox(\n","            bbox, (o_H, o_W), x_flip=params['x_flip'])\n","\n","        return img, bbox, label, scale\n","\n","\n","class Dataset:\n","    def __init__(self, opt):\n","        self.opt = opt\n","        self.db = VOCBboxDataset(opt.voc_data_dir)\n","        self.tsf = Transform(opt.min_size, opt.max_size)\n","\n","    def __getitem__(self, idx):\n","        ori_img, bbox, label, difficult = self.db.get_example(idx)\n","\n","        img, bbox, label, scale = self.tsf((ori_img, bbox, label))\n","        # TODO: check whose stride is negative to fix this instead copy all\n","        # some of the strides of a given numpy array are negative.\n","        return img.copy(), bbox.copy(), label.copy(), scale\n","\n","    def __len__(self):\n","        return len(self.db)\n","\n","\n","class TestDataset:\n","    def __init__(self, opt, split='test', use_difficult=True):\n","        self.opt = opt\n","        self.db = VOCBboxDataset(opt.voc_data_dir, split=split, use_difficult=use_difficult)\n","\n","    def __getitem__(self, idx):\n","        ori_img, bbox, label, difficult = self.db.get_example(idx)\n","        img = preprocess(ori_img)\n","        return img, ori_img.shape[1:], bbox, label, difficult\n","\n","    def __len__(self):\n","        return len(self.db)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RIJbuIM7NFY2","colab_type":"text"},"source":["## SVHN dataset"]},{"cell_type":"code","metadata":{"id":"fHJzw9U_NI--","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":223},"outputId":"83f80dbf-2e0c-4a69-c335-f7e096d2c8bb","executionInfo":{"status":"ok","timestamp":1559029417585,"user_tz":-480,"elapsed":3350,"user":{"displayName":"张耿","photoUrl":"","userId":"17787706570393703878"}}},"source":["import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from skimage import io\n","import skimage.transform as sktransform\n","import matplotlib.pylab as plt\n","import json\n","\n","class Transform():\n","    \n","    def __init__(self, min_size=600, max_size=1000):\n","        self.min_size = min_size\n","        self.max_size = max_size\n","        \n","    def __call__(self, data):\n","        img, bbox = data\n","        \n","        img = img.transpose(2, 0, 1)\n","        img = img/255.\n","        # img resize\n","        c, h, w = img.shape\n","        smin, smax = self.min_size/min(h, w), self.max_size/max(h, w)\n","        scale = min(smin, smax)\n","        img = sktransform.resize(img, (c, h*scale, w*scale))\n","        \n","        # img normalize\n","        img = transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])(torch.from_numpy(img).float()).numpy().astype(np.float32)\n","        \n","        # bbox resize\n","        bbox = bbox * scale\n","        bbox = bbox[:, (1, 0, 3, 2)]\n","        bbox[:, 2] = bbox[:, 0] + bbox[:, 2]\n","        bbox[:, 3] = bbox[:, 1] + bbox[:, 3]\n","        return img, bbox, scale\n","\n","\n","class TrainValSet(Dataset):\n","\n","    def __init__(self, root, info='digitStruct.json'):\n","        self.root = root\n","        info_path = root+'/'+info\n","        with open(info_path, 'r') as f:\n","            self.data_info = json.load(f)\n","        self.transform = Transform()\n","    \n","    def __len__(self):\n","        return len(self.data_info)\n","    \n","    def __getitem__(self, idx):\n","        img_path = self.root+'/'+self.data_info[idx]['name']\n","        img = io.imread(img_path)\n","        \n","        bbox = np.asarray(self.data_info[idx]['bbox']).astype(np.float32)\n","        label = np.asarray(self.data_info[idx]['label']).astype(np.int32)\n","        label[label==10]=0 # 0 is labeled as 10 in svhn \n","        \n","        img, bbox, scale = self.transform((img, bbox))\n","        \n","        return img, bbox, label, scale\n","\n","\n","def denormalize(img):\n","    return (img * 0.225 + 0.45).clip(0, 1) \n","\n","\n","def show_item(item):\n","    img, bbox, label, scale = item\n","    img = img.transpose(1, 2, 0)\n","    img = denormalize(img)\n","    \n","    title = 'Label: '+ str(label)+\" shape: \" + str(img.shape)\n","    \n","    plt.figure()\n","    plt.title(title)\n","    plt.imshow(img)\n","    ax = plt.gca()\n","    for i in range(bbox.shape[0]):\n","        xy = (bbox[i, 1], bbox[i, 0])\n","        w, h = bbox[i, 3]-bbox[i, 1], bbox[i, 2]-bbox[i, 0]\n","        rect = plt.Rectangle(xy, w, h, fill=False, edgecolor = 'red',linewidth=1)\n","        ax.add_patch(rect)\n","    plt.axis('off')\n","    plt.show()\n","\n","\n","test = TrainValSet('gdrive/My Drive/svhn/data/train')\n","show_item(test[np.random.randint(0, len(test))])"],"execution_count":28,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYMAAADOCAYAAADc1W4UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoNNmzZt2kQbDDZt2rRpExH9A2JUh1bS3NtlAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"PALLEpfb0Cha","colab_type":"text"},"source":["## Test Dataset"]},{"cell_type":"code","metadata":{"id":"IipKhztMyRhi","colab_type":"code","outputId":"99e10948-bc65-4ee8-a67e-77e04adaffbc","executionInfo":{"status":"ok","timestamp":1559024457699,"user_tz":-480,"elapsed":4351,"user":{"displayName":"张耿","photoUrl":"","userId":"17787706570393703878"}},"colab":{"base_uri":"https://localhost:8080/","height":161}},"source":["from torch.utils.data import DataLoader\n","\n","\n","dataset = Dataset(opt)\n","img, bbox, label, scale = dataset[731]\n","print(img.shape, img.dtype)\n","print(bbox.shape, bbox.dtype)\n","print(label.shape, label.dtype)\n","print(scale, type(scale))\n","\n","dataloader = DataLoader(dataset, batch_size=1, \n","                        shuffle=True, num_workers=opt.num_workers)\n","\n","dataiter = iter(dataloader)\n","img, bbox_, label_, scale = dataiter.next()\n","print(img.shape, img.dtype)\n","print(bbox_.shape, bbox_.dtype)\n","print(label_.shape, label_.dtype)\n","print(scale.shape, scale.dtype)\n","\n","testset = TestDataset(opt)\n","test_dataloader = DataLoader(testset, batch_size=1, \n","                             num_workers=opt.test_num_workers,\n","                             shuffle=False, pin_memory=True)\n","\n","dataiter = iter(test_dataloader)\n","imgs, sizes, gt_bboxes_, gt_labels_, gt_difficults_ = dataiter.next()\n","sizes = [sizes[0][0].item(), sizes[1][0].item()]\n","for img_, size_ in zip(imgs, [sizes]):\n","    print(img_.shape)\n","    print(size_)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["(3, 600, 829) float32\n","(2, 4) float32\n","(2,) int32\n","1.6574585635359116 <class 'float'>\n","torch.Size([1, 3, 600, 904]) torch.float32\n","torch.Size([1, 1, 4]) torch.float32\n","torch.Size([1, 1]) torch.int32\n","torch.Size([1]) torch.float64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o1TBf3zZyw0T","colab_type":"text"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"hCbrtq5y0yEa","colab_type":"text"},"source":["## model.utils.bbox_tools"]},{"cell_type":"code","metadata":{"id":"JkYzXq3q02OM","colab_type":"code","colab":{}},"source":["import numpy as np\n","import numpy as xp\n","\n","\n","def loc2bbox(src_bbox, loc):\n","    \"\"\"Decode bounding boxes from bounding box offsets and scales.\n","\n","    Given bounding box offsets and scales computed by\n","    :meth:`bbox2loc`, this function decodes the representation to\n","    coordinates in 2D image coordinates.\n","\n","    Given scales and offsets :math:`t_y, t_x, t_h, t_w` and a bounding\n","    box whose center is :math:`(y, x) = p_y, p_x` and size :math:`p_h, p_w`,\n","    the decoded bounding box's center :math:`\\\\hat{g}_y`, :math:`\\\\hat{g}_x`\n","    and size :math:`\\\\hat{g}_h`, :math:`\\\\hat{g}_w` are calculated\n","    by the following formulas.\n","\n","    * :math:`\\\\hat{g}_y = p_h t_y + p_y`\n","    * :math:`\\\\hat{g}_x = p_w t_x + p_x`\n","    * :math:`\\\\hat{g}_h = p_h \\\\exp(t_h)`\n","    * :math:`\\\\hat{g}_w = p_w \\\\exp(t_w)`\n","\n","    The decoding formulas are used in works such as R-CNN [#]_.\n","\n","    The output is same type as the type of the inputs.\n","\n","    .. [#] Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. \\\n","    Rich feature hierarchies for accurate object detection and semantic \\\n","    segmentation. CVPR 2014.\n","\n","    Args:\n","        src_bbox (array): A coordinates of bounding boxes.\n","            Its shape is :math:`(R, 4)`. These coordinates are\n","            :math:`p_{ymin}, p_{xmin}, p_{ymax}, p_{xmax}`.\n","        loc (array): An array with offsets and scales.\n","            The shapes of :obj:`src_bbox` and :obj:`loc` should be same.\n","            This contains values :math:`t_y, t_x, t_h, t_w`.\n","\n","    Returns:\n","        array:\n","        Decoded bounding box coordinates. Its shape is :math:`(R, 4)`. \\\n","        The second axis contains four values \\\n","        :math:`\\\\hat{g}_{ymin}, \\\\hat{g}_{xmin},\n","        \\\\hat{g}_{ymax}, \\\\hat{g}_{xmax}`.\n","\n","    \"\"\"\n","\n","    if src_bbox.shape[0] == 0:\n","        return xp.zeros((0, 4), dtype=loc.dtype)\n","\n","    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n","\n","    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n","    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n","    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n","    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n","\n","    dy = loc[:, 0::4]\n","    dx = loc[:, 1::4]\n","    dh = loc[:, 2::4]\n","    dw = loc[:, 3::4]\n","\n","    ctr_y = dy * src_height[:, xp.newaxis] + src_ctr_y[:, xp.newaxis]\n","    ctr_x = dx * src_width[:, xp.newaxis] + src_ctr_x[:, xp.newaxis]\n","    h = xp.exp(dh) * src_height[:, xp.newaxis]\n","    w = xp.exp(dw) * src_width[:, xp.newaxis]\n","\n","    dst_bbox = xp.zeros(loc.shape, dtype=loc.dtype)\n","    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n","    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n","    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n","    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n","\n","    return dst_bbox\n","\n","\n","def bbox2loc(src_bbox, dst_bbox):\n","    \"\"\"Encodes the source and the destination bounding boxes to \"loc\".\n","\n","    Given bounding boxes, this function computes offsets and scales\n","    to match the source bounding boxes to the target bounding boxes.\n","    Mathematcially, given a bounding box whose center is\n","    :math:`(y, x) = p_y, p_x` and\n","    size :math:`p_h, p_w` and the target bounding box whose center is\n","    :math:`g_y, g_x` and size :math:`g_h, g_w`, the offsets and scales\n","    :math:`t_y, t_x, t_h, t_w` can be computed by the following formulas.\n","\n","    * :math:`t_y = \\\\frac{(g_y - p_y)} {p_h}`\n","    * :math:`t_x = \\\\frac{(g_x - p_x)} {p_w}`\n","    * :math:`t_h = \\\\log(\\\\frac{g_h} {p_h})`\n","    * :math:`t_w = \\\\log(\\\\frac{g_w} {p_w})`\n","\n","    The output is same type as the type of the inputs.\n","    The encoding formulas are used in works such as R-CNN [#]_.\n","\n","    .. [#] Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. \\\n","    Rich feature hierarchies for accurate object detection and semantic \\\n","    segmentation. CVPR 2014.\n","\n","    Args:\n","        src_bbox (array): An image coordinate array whose shape is\n","            :math:`(R, 4)`. :math:`R` is the number of bounding boxes.\n","            These coordinates are\n","            :math:`p_{ymin}, p_{xmin}, p_{ymax}, p_{xmax}`.\n","        dst_bbox (array): An image coordinate array whose shape is\n","            :math:`(R, 4)`.\n","            These coordinates are\n","            :math:`g_{ymin}, g_{xmin}, g_{ymax}, g_{xmax}`.\n","\n","    Returns:\n","        array:\n","        Bounding box offsets and scales from :obj:`src_bbox` \\\n","        to :obj:`dst_bbox`. \\\n","        This has shape :math:`(R, 4)`.\n","        The second axis contains four values :math:`t_y, t_x, t_h, t_w`.\n","\n","    \"\"\"\n","\n","    height = src_bbox[:, 2] - src_bbox[:, 0]\n","    width = src_bbox[:, 3] - src_bbox[:, 1]\n","    ctr_y = src_bbox[:, 0] + 0.5 * height\n","    ctr_x = src_bbox[:, 1] + 0.5 * width\n","\n","    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]\n","    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]\n","    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height\n","    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width\n","\n","    eps = xp.finfo(height.dtype).eps\n","    height = xp.maximum(height, eps)\n","    width = xp.maximum(width, eps)\n","\n","    dy = (base_ctr_y - ctr_y) / height\n","    dx = (base_ctr_x - ctr_x) / width\n","    dh = xp.log(base_height / height)\n","    dw = xp.log(base_width / width)\n","\n","    loc = xp.vstack((dy, dx, dh, dw)).transpose()\n","    return loc\n","\n","\n","def bbox_iou(bbox_a, bbox_b):\n","    \"\"\"Calculate the Intersection of Unions (IoUs) between bounding boxes.\n","\n","    IoU is calculated as a ratio of area of the intersection\n","    and area of the union.\n","\n","    This function accepts both :obj:`numpy.ndarray` and :obj:`cupy.ndarray` as\n","    inputs. Please note that both :obj:`bbox_a` and :obj:`bbox_b` need to be\n","    same type.\n","    The output is same type as the type of the inputs.\n","\n","    Args:\n","        bbox_a (array): An array whose shape is :math:`(N, 4)`.\n","            :math:`N` is the number of bounding boxes.\n","            The dtype should be :obj:`numpy.float32`.\n","        bbox_b (array): An array similar to :obj:`bbox_a`,\n","            whose shape is :math:`(K, 4)`.\n","            The dtype should be :obj:`numpy.float32`.\n","\n","    Returns:\n","        array:\n","        An array whose shape is :math:`(N, K)`. \\\n","        An element at index :math:`(n, k)` contains IoUs between \\\n","        :math:`n` th bounding box in :obj:`bbox_a` and :math:`k` th bounding \\\n","        box in :obj:`bbox_b`.\n","\n","    \"\"\"\n","    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n","        raise IndexError\n","\n","    # top left\n","    tl = xp.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n","    # bottom right\n","    br = xp.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n","\n","    area_i = xp.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n","    area_a = xp.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n","    area_b = xp.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n","    return area_i / (area_a[:, None] + area_b - area_i)\n","\n","\n","def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2],\n","                         anchor_scales=[8, 16, 32]):\n","    \"\"\"Generate anchor base windows by enumerating aspect ratio and scales.\n","\n","    Generate anchors that are scaled and modified to the given aspect ratios.\n","    Area of a scaled anchor is preserved when modifying to the given aspect\n","    ratio.\n","\n","    :obj:`R = len(ratios) * len(anchor_scales)` anchors are generated by this\n","    function.\n","    The :obj:`i * len(anchor_scales) + j` th anchor corresponds to an anchor\n","    generated by :obj:`ratios[i]` and :obj:`anchor_scales[j]`.\n","\n","    For example, if the scale is :math:`8` and the ratio is :math:`0.25`,\n","    the width and the height of the base window will be stretched by :math:`8`.\n","    For modifying the anchor to the given aspect ratio,\n","    the height is halved and the width is doubled.\n","\n","    Args:\n","        base_size (number): The width and the height of the reference window.\n","        ratios (list of floats): This is ratios of width to height of\n","            the anchors.\n","        anchor_scales (list of numbers): This is areas of anchors.\n","            Those areas will be the product of the square of an element in\n","            :obj:`anchor_scales` and the original area of the reference\n","            window.\n","\n","    Returns:\n","        ~numpy.ndarray:\n","        An array of shape :math:`(R, 4)`.\n","        Each element is a set of coordinates of a bounding box.\n","        The second axis corresponds to\n","        :math:`(y_{min}, x_{min}, y_{max}, x_{max})` of a bounding box.\n","\n","    \"\"\"\n","    py = base_size / 2.\n","    px = base_size / 2.\n","\n","    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4),\n","                           dtype=np.float32)\n","    for i in range(len(ratios)):\n","        for j in range(len(anchor_scales)):\n","            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n","            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n","\n","            index = i * len(anchor_scales) + j\n","            anchor_base[index, 0] = py - h / 2.\n","            anchor_base[index, 1] = px - w / 2.\n","            anchor_base[index, 2] = py + h / 2.\n","            anchor_base[index, 3] = px + w / 2.\n","    return anchor_base\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4lKQyzZk1U1p","colab_type":"text"},"source":["## model.utils.nms.no_maximum_suppression"]},{"cell_type":"code","metadata":{"id":"6wInKYwgrsgd","colab_type":"code","colab":{}},"source":["\n","import numpy as np\n","\n","def _nms_gpu_post( mask,\n","                  n_bbox,\n","                   threads_per_block,\n","                   col_blocks\n","                  ):\n","    n_selection = 0\n","    one_ull = np.array([1],dtype=np.uint64)\n","    selection = np.zeros((n_bbox,), dtype=np.int32)\n","    remv = np.zeros((col_blocks,), dtype=np.uint64)\n","\n","    for i in range(n_bbox):\n","        nblock = i // threads_per_block\n","        inblock = i % threads_per_block\n","\n","        if not (remv[nblock] & one_ull << inblock):\n","            selection[n_selection] = i\n","            n_selection += 1\n","\n","            index = i * col_blocks\n","            for j in range(nblock, col_blocks):\n","                remv[j] |= mask[index + j]\n","    return selection, n_selection\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"no_dn_S71aPZ","colab_type":"code","colab":{}},"source":["#from __future__ import division\n","import numpy as np\n","import cupy as cp\n","import torch as t\n","# try:\n","#     from ._nms_gpu_post import _nms_gpu_post\n","# except:\n","#     import warnings\n","#     warnings.warn('''\n","#     the python code for non_maximum_suppression is about 2x slow\n","#     It is strongly recommended to build cython code: \n","#     `cd model/utils/nms/; python3 build.py build_ext --inplace''')\n","#     from ._nms_gpu_post_py import _nms_gpu_post\n","\n","\n","@cp.util.memoize(for_each_device=True)\n","def _load_kernel(kernel_name, code, options=()):\n","    cp.cuda.runtime.free(0)\n","    assert isinstance(options, tuple)\n","    kernel_code = cp.cuda.compile_with_cache(code, options=options)\n","    return kernel_code.get_function(kernel_name)\n","\n","\n","def non_maximum_suppression(bbox, thresh, score=None,\n","                            limit=None):\n","    \"\"\"Suppress bounding boxes according to their IoUs.\n","\n","    This method checks each bounding box sequentially and selects the bounding\n","    box if the Intersection over Unions (IoUs) between the bounding box and the\n","    previously selected bounding boxes is less than :obj:`thresh`. This method\n","    is mainly used as postprocessing of object detection.\n","    The bounding boxes are selected from ones with higher scores.\n","    If :obj:`score` is not provided as an argument, the bounding box\n","    is ordered by its index in ascending order.\n","\n","    The bounding boxes are expected to be packed into a two dimensional\n","    tensor of shape :math:`(R, 4)`, where :math:`R` is the number of\n","    bounding boxes in the image. The second axis represents attributes of\n","    the bounding box. They are :math:`(y_{min}, x_{min}, y_{max}, x_{max})`,\n","    where the four attributes are coordinates of the top left and the\n","    bottom right vertices.\n","\n","    :obj:`score` is a float array of shape :math:`(R,)`. Each score indicates\n","    confidence of prediction.\n","\n","    This function accepts both :obj:`numpy.ndarray` and :obj:`cupy.ndarray` as\n","    an input. Please note that both :obj:`bbox` and :obj:`score` need to be\n","    the same type.\n","    The type of the output is the same as the input.\n","\n","    Args:\n","        bbox (array): Bounding boxes to be transformed. The shape is\n","            :math:`(R, 4)`. :math:`R` is the number of bounding boxes.\n","        thresh (float): Threshold of IoUs.\n","        score (array): An array of confidences whose shape is :math:`(R,)`.\n","        limit (int): The upper bound of the number of the output bounding\n","            boxes. If it is not specified, this method selects as many\n","            bounding boxes as possible.\n","\n","    Returns:\n","        array:\n","        An array with indices of bounding boxes that are selected. \\\n","        They are sorted by the scores of bounding boxes in descending \\\n","        order. \\\n","        The shape of this array is :math:`(K,)` and its dtype is\\\n","        :obj:`numpy.int32`. Note that :math:`K \\\\leq R`.\n","\n","    \"\"\"\n","\n","    return _non_maximum_suppression_gpu(bbox, thresh, score, limit)\n","\n","\n","def _non_maximum_suppression_gpu(bbox, thresh, score=None, limit=None):\n","    if len(bbox) == 0:\n","        return cp.zeros((0,), dtype=np.int32)\n","\n","    n_bbox = bbox.shape[0]\n","\n","    if score is not None:\n","        order = score.argsort()[::-1].astype(np.int32)\n","    else:\n","        order = cp.arange(n_bbox, dtype=np.int32)\n","\n","    sorted_bbox = bbox[order, :]\n","    selec, n_selec = _call_nms_kernel(\n","        sorted_bbox, thresh)\n","    selec = selec[:n_selec]\n","    selec = order[selec]\n","    if limit is not None:\n","        selec = selec[:limit]\n","    return cp.asnumpy(selec)\n","\n","\n","_nms_gpu_code = '''\n","#define DIVUP(m,n) ((m) / (n) + ((m) % (n) > 0))\n","int const threadsPerBlock = sizeof(unsigned long long) * 8;\n","\n","__device__\n","inline float devIoU(float const *const bbox_a, float const *const bbox_b) {\n","  float top = max(bbox_a[0], bbox_b[0]);\n","  float bottom = min(bbox_a[2], bbox_b[2]);\n","  float left = max(bbox_a[1], bbox_b[1]);\n","  float right = min(bbox_a[3], bbox_b[3]);\n","  float height = max(bottom - top, 0.f);\n","  float width = max(right - left, 0.f);\n","  float area_i = height * width;\n","  float area_a = (bbox_a[2] - bbox_a[0]) * (bbox_a[3] - bbox_a[1]);\n","  float area_b = (bbox_b[2] - bbox_b[0]) * (bbox_b[3] - bbox_b[1]);\n","  return area_i / (area_a + area_b - area_i);\n","}\n","\n","extern \"C\"\n","__global__\n","void nms_kernel(const int n_bbox, const float thresh,\n","                const float *dev_bbox,\n","                unsigned long long *dev_mask) {\n","  const int row_start = blockIdx.y;\n","  const int col_start = blockIdx.x;\n","\n","  const int row_size =\n","        min(n_bbox - row_start * threadsPerBlock, threadsPerBlock);\n","  const int col_size =\n","        min(n_bbox - col_start * threadsPerBlock, threadsPerBlock);\n","\n","  __shared__ float block_bbox[threadsPerBlock * 4];\n","  if (threadIdx.x < col_size) {\n","    block_bbox[threadIdx.x * 4 + 0] =\n","        dev_bbox[(threadsPerBlock * col_start + threadIdx.x) * 4 + 0];\n","    block_bbox[threadIdx.x * 4 + 1] =\n","        dev_bbox[(threadsPerBlock * col_start + threadIdx.x) * 4 + 1];\n","    block_bbox[threadIdx.x * 4 + 2] =\n","        dev_bbox[(threadsPerBlock * col_start + threadIdx.x) * 4 + 2];\n","    block_bbox[threadIdx.x * 4 + 3] =\n","        dev_bbox[(threadsPerBlock * col_start + threadIdx.x) * 4 + 3];\n","  }\n","  __syncthreads();\n","\n","  if (threadIdx.x < row_size) {\n","    const int cur_box_idx = threadsPerBlock * row_start + threadIdx.x;\n","    const float *cur_box = dev_bbox + cur_box_idx * 4;\n","    int i = 0;\n","    unsigned long long t = 0;\n","    int start = 0;\n","    if (row_start == col_start) {\n","      start = threadIdx.x + 1;\n","    }\n","    for (i = start; i < col_size; i++) {\n","      if (devIoU(cur_box, block_bbox + i * 4) >= thresh) {\n","        t |= 1ULL << i;\n","      }\n","    }\n","    const int col_blocks = DIVUP(n_bbox, threadsPerBlock);\n","    dev_mask[cur_box_idx * col_blocks + col_start] = t;\n","  }\n","}\n","'''\n","\n","\n","def _call_nms_kernel(bbox, thresh):\n","    # PyTorch does not support unsigned long Tensor.\n","    # Doesn't matter,since it returns ndarray finally.\n","    # So I'll keep it unmodified.\n","    n_bbox = bbox.shape[0]\n","    threads_per_block = 64\n","    col_blocks = np.ceil(n_bbox / threads_per_block).astype(np.int32)\n","    blocks = (col_blocks, col_blocks, 1)\n","    threads = (threads_per_block, 1, 1)\n","\n","    mask_dev = cp.zeros((n_bbox * col_blocks,), dtype=np.uint64)\n","    bbox = cp.ascontiguousarray(bbox, dtype=np.float32)\n","    kern = _load_kernel('nms_kernel', _nms_gpu_code)\n","    kern(blocks, threads, args=(cp.int32(n_bbox), cp.float32(thresh),\n","                                bbox, mask_dev))\n","\n","    mask_host = mask_dev.get()\n","    selection, n_selec = _nms_gpu_post(\n","        mask_host, n_bbox, threads_per_block, col_blocks)\n","    return selection, n_selec\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Df26pH221DlS","colab_type":"text"},"source":["## model.utils.creator_tool"]},{"cell_type":"code","metadata":{"id":"eLpPPDJh1G4a","colab_type":"code","colab":{}},"source":["import numpy as np\n","import cupy as cp\n","\n","#from model.utils.bbox_tools import bbox2loc, bbox_iou, loc2bbox\n","#from model.utils.nms import non_maximum_suppression\n","\n","\n","class ProposalTargetCreator(object):\n","    \"\"\"Assign ground truth bounding boxes to given RoIs.\n","\n","    The :meth:`__call__` of this class generates training targets\n","    for each object proposal.\n","    This is used to train Faster RCNN [#]_.\n","\n","    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n","    Faster R-CNN: Towards Real-Time Object Detection with \\\n","    Region Proposal Networks. NIPS 2015.\n","\n","    Args:\n","        n_sample (int): The number of sampled regions.\n","        pos_ratio (float): Fraction of regions that is labeled as a\n","            foreground.\n","        pos_iou_thresh (float): IoU threshold for a RoI to be considered as a\n","            foreground.\n","        neg_iou_thresh_hi (float): RoI is considered to be the background\n","            if IoU is in\n","            [:obj:`neg_iou_thresh_hi`, :obj:`neg_iou_thresh_hi`).\n","        neg_iou_thresh_lo (float): See above.\n","\n","    \"\"\"\n","\n","    def __init__(self,\n","                 n_sample=128,\n","                 pos_ratio=0.25, pos_iou_thresh=0.5,\n","                 neg_iou_thresh_hi=0.5, neg_iou_thresh_lo=0.0\n","                 ):\n","        self.n_sample = n_sample\n","        self.pos_ratio = pos_ratio\n","        self.pos_iou_thresh = pos_iou_thresh\n","        self.neg_iou_thresh_hi = neg_iou_thresh_hi\n","        self.neg_iou_thresh_lo = neg_iou_thresh_lo  # NOTE:default 0.1 in py-faster-rcnn\n","\n","    def __call__(self, roi, bbox, label,\n","                 loc_normalize_mean=(0., 0., 0., 0.),\n","                 loc_normalize_std=(0.1, 0.1, 0.2, 0.2)):\n","        \"\"\"Assigns ground truth to sampled proposals.\n","\n","        This function samples total of :obj:`self.n_sample` RoIs\n","        from the combination of :obj:`roi` and :obj:`bbox`.\n","        The RoIs are assigned with the ground truth class labels as well as\n","        bounding box offsets and scales to match the ground truth bounding\n","        boxes. As many as :obj:`pos_ratio * self.n_sample` RoIs are\n","        sampled as foregrounds.\n","\n","        Offsets and scales of bounding boxes are calculated using\n","        :func:`model.utils.bbox_tools.bbox2loc`.\n","        Also, types of input arrays and output arrays are same.\n","\n","        Here are notations.\n","\n","        * :math:`S` is the total number of sampled RoIs, which equals \\\n","            :obj:`self.n_sample`.\n","        * :math:`L` is number of object classes possibly including the \\\n","            background.\n","\n","        Args:\n","            roi (array): Region of Interests (RoIs) from which we sample.\n","                Its shape is :math:`(R, 4)`\n","            bbox (array): The coordinates of ground truth bounding boxes.\n","                Its shape is :math:`(R', 4)`.\n","            label (array): Ground truth bounding box labels. Its shape\n","                is :math:`(R',)`. Its range is :math:`[0, L - 1]`, where\n","                :math:`L` is the number of foreground classes.\n","            loc_normalize_mean (tuple of four floats): Mean values to normalize\n","                coordinates of bouding boxes.\n","            loc_normalize_std (tupler of four floats): Standard deviation of\n","                the coordinates of bounding boxes.\n","\n","        Returns:\n","            (array, array, array):\n","\n","            * **sample_roi**: Regions of interests that are sampled. \\\n","                Its shape is :math:`(S, 4)`.\n","            * **gt_roi_loc**: Offsets and scales to match \\\n","                the sampled RoIs to the ground truth bounding boxes. \\\n","                Its shape is :math:`(S, 4)`.\n","            * **gt_roi_label**: Labels assigned to sampled RoIs. Its shape is \\\n","                :math:`(S,)`. Its range is :math:`[0, L]`. The label with \\\n","                value 0 is the background.\n","\n","        \"\"\"\n","        n_bbox, _ = bbox.shape\n","\n","        roi = np.concatenate((roi, bbox), axis=0)\n","\n","        pos_roi_per_image = np.round(self.n_sample * self.pos_ratio)\n","        iou = bbox_iou(roi, bbox)\n","        gt_assignment = iou.argmax(axis=1)\n","        max_iou = iou.max(axis=1)\n","        # Offset range of classes from [0, n_fg_class - 1] to [1, n_fg_class].\n","        # The label with value 0 is the background.\n","        gt_roi_label = label[gt_assignment] + 1\n","\n","        # Select foreground RoIs as those with >= pos_iou_thresh IoU.\n","        pos_index = np.where(max_iou >= self.pos_iou_thresh)[0]\n","        pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n","        if pos_index.size > 0:\n","            pos_index = np.random.choice(\n","                pos_index, size=pos_roi_per_this_image, replace=False)\n","\n","        # Select background RoIs as those within\n","        # [neg_iou_thresh_lo, neg_iou_thresh_hi).\n","        neg_index = np.where((max_iou < self.neg_iou_thresh_hi) &\n","                             (max_iou >= self.neg_iou_thresh_lo))[0]\n","        neg_roi_per_this_image = self.n_sample - pos_roi_per_this_image\n","        neg_roi_per_this_image = int(min(neg_roi_per_this_image,\n","                                         neg_index.size))\n","        if neg_index.size > 0:\n","            neg_index = np.random.choice(\n","                neg_index, size=neg_roi_per_this_image, replace=False)\n","\n","        # The indices that we're selecting (both positive and negative).\n","        keep_index = np.append(pos_index, neg_index)\n","        gt_roi_label = gt_roi_label[keep_index]\n","        gt_roi_label[pos_roi_per_this_image:] = 0  # negative labels --> 0\n","        sample_roi = roi[keep_index]\n","\n","        # Compute offsets and scales to match sampled RoIs to the GTs.\n","        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]])\n","        gt_roi_loc = ((gt_roi_loc - np.array(loc_normalize_mean, np.float32)\n","                       ) / np.array(loc_normalize_std, np.float32))\n","\n","        return sample_roi, gt_roi_loc, gt_roi_label\n","\n","\n","class AnchorTargetCreator(object):\n","    \"\"\"Assign the ground truth bounding boxes to anchors.\n","\n","    Assigns the ground truth bounding boxes to anchors for training Region\n","    Proposal Networks introduced in Faster R-CNN [#]_.\n","\n","    Offsets and scales to match anchors to the ground truth are\n","    calculated using the encoding scheme of\n","    :func:`model.utils.bbox_tools.bbox2loc`.\n","\n","    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n","    Faster R-CNN: Towards Real-Time Object Detection with \\\n","    Region Proposal Networks. NIPS 2015.\n","\n","    Args:\n","        n_sample (int): The number of regions to produce.\n","        pos_iou_thresh (float): Anchors with IoU above this\n","            threshold will be assigned as positive.\n","        neg_iou_thresh (float): Anchors with IoU below this\n","            threshold will be assigned as negative.\n","        pos_ratio (float): Ratio of positive regions in the\n","            sampled regions.\n","\n","    \"\"\"\n","\n","    def __init__(self,\n","                 n_sample=256,\n","                 pos_iou_thresh=0.7, neg_iou_thresh=0.3,\n","                 pos_ratio=0.5):\n","        self.n_sample = n_sample\n","        self.pos_iou_thresh = pos_iou_thresh\n","        self.neg_iou_thresh = neg_iou_thresh\n","        self.pos_ratio = pos_ratio\n","\n","    def __call__(self, bbox, anchor, img_size):\n","        \"\"\"Assign ground truth supervision to sampled subset of anchors.\n","\n","        Types of input arrays and output arrays are same.\n","\n","        Here are notations.\n","\n","        * :math:`S` is the number of anchors.\n","        * :math:`R` is the number of bounding boxes.\n","\n","        Args:\n","            bbox (array): Coordinates of bounding boxes. Its shape is\n","                :math:`(R, 4)`.\n","            anchor (array): Coordinates of anchors. Its shape is\n","                :math:`(S, 4)`.\n","            img_size (tuple of ints): A tuple :obj:`H, W`, which\n","                is a tuple of height and width of an image.\n","\n","        Returns:\n","            (array, array):\n","\n","            #NOTE: it's scale not only  offset\n","            * **loc**: Offsets and scales to match the anchors to \\\n","                the ground truth bounding boxes. Its shape is :math:`(S, 4)`.\n","            * **label**: Labels of anchors with values \\\n","                :obj:`(1=positive, 0=negative, -1=ignore)`. Its shape \\\n","                is :math:`(S,)`.\n","\n","        \"\"\"\n","\n","        img_H, img_W = img_size\n","\n","        n_anchor = len(anchor)\n","        inside_index = _get_inside_index(anchor, img_H, img_W)\n","        anchor = anchor[inside_index]\n","        argmax_ious, label = self._create_label(\n","            inside_index, anchor, bbox)\n","\n","        # compute bounding box regression targets\n","        loc = bbox2loc(anchor, bbox[argmax_ious])\n","\n","        # map up to original set of anchors\n","        label = _unmap(label, n_anchor, inside_index, fill=-1)\n","        loc = _unmap(loc, n_anchor, inside_index, fill=0)\n","\n","        return loc, label\n","\n","    def _create_label(self, inside_index, anchor, bbox):\n","        # label: 1 is positive, 0 is negative, -1 is dont care\n","        label = np.empty((len(inside_index),), dtype=np.int32)\n","        label.fill(-1)\n","\n","        argmax_ious, max_ious, gt_argmax_ious = \\\n","            self._calc_ious(anchor, bbox, inside_index)\n","\n","        # assign negative labels first so that positive labels can clobber them\n","        label[max_ious < self.neg_iou_thresh] = 0\n","\n","        # positive label: for each gt, anchor with highest iou\n","        label[gt_argmax_ious] = 1\n","\n","        # positive label: above threshold IOU\n","        label[max_ious >= self.pos_iou_thresh] = 1\n","\n","        # subsample positive labels if we have too many\n","        n_pos = int(self.pos_ratio * self.n_sample)\n","        pos_index = np.where(label == 1)[0]\n","        if len(pos_index) > n_pos:\n","            disable_index = np.random.choice(\n","                pos_index, size=(len(pos_index) - n_pos), replace=False)\n","            label[disable_index] = -1\n","\n","        # subsample negative labels if we have too many\n","        n_neg = self.n_sample - np.sum(label == 1)\n","        neg_index = np.where(label == 0)[0]\n","        if len(neg_index) > n_neg:\n","            disable_index = np.random.choice(\n","                neg_index, size=(len(neg_index) - n_neg), replace=False)\n","            label[disable_index] = -1\n","\n","        return argmax_ious, label\n","\n","    def _calc_ious(self, anchor, bbox, inside_index):\n","        # ious between the anchors and the gt boxes\n","        ious = bbox_iou(anchor, bbox)\n","        argmax_ious = ious.argmax(axis=1)\n","        max_ious = ious[np.arange(len(inside_index)), argmax_ious]\n","        gt_argmax_ious = ious.argmax(axis=0)\n","        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n","        gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n","\n","        return argmax_ious, max_ious, gt_argmax_ious\n","\n","\n","def _unmap(data, count, index, fill=0):\n","    # Unmap a subset of item (data) back to the original set of items (of\n","    # size count)\n","\n","    if len(data.shape) == 1:\n","        ret = np.empty((count,), dtype=data.dtype)\n","        ret.fill(fill)\n","        ret[index] = data\n","    else:\n","        ret = np.empty((count,) + data.shape[1:], dtype=data.dtype)\n","        ret.fill(fill)\n","        ret[index, :] = data\n","    return ret\n","\n","\n","def _get_inside_index(anchor, H, W):\n","    # Calc indicies of anchors which are located completely inside of the image\n","    # whose size is speficied.\n","    index_inside = np.where(\n","        (anchor[:, 0] >= 0) &\n","        (anchor[:, 1] >= 0) &\n","        (anchor[:, 2] <= H) &\n","        (anchor[:, 3] <= W)\n","    )[0]\n","    return index_inside\n","\n","\n","class ProposalCreator:\n","    # unNOTE: I'll make it undifferential\n","    # unTODO: make sure it's ok\n","    # It's ok\n","    \"\"\"Proposal regions are generated by calling this object.\n","\n","    The :meth:`__call__` of this object outputs object detection proposals by\n","    applying estimated bounding box offsets\n","    to a set of anchors.\n","\n","    This class takes parameters to control number of bounding boxes to\n","    pass to NMS and keep after NMS.\n","    If the paramters are negative, it uses all the bounding boxes supplied\n","    or keep all the bounding boxes returned by NMS.\n","\n","    This class is used for Region Proposal Networks introduced in\n","    Faster R-CNN [#]_.\n","\n","    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n","    Faster R-CNN: Towards Real-Time Object Detection with \\\n","    Region Proposal Networks. NIPS 2015.\n","\n","    Args:\n","        nms_thresh (float): Threshold value used when calling NMS.\n","        n_train_pre_nms (int): Number of top scored bounding boxes\n","            to keep before passing to NMS in train mode.\n","        n_train_post_nms (int): Number of top scored bounding boxes\n","            to keep after passing to NMS in train mode.\n","        n_test_pre_nms (int): Number of top scored bounding boxes\n","            to keep before passing to NMS in test mode.\n","        n_test_post_nms (int): Number of top scored bounding boxes\n","            to keep after passing to NMS in test mode.\n","        force_cpu_nms (bool): If this is :obj:`True`,\n","            always use NMS in CPU mode. If :obj:`False`,\n","            the NMS mode is selected based on the type of inputs.\n","        min_size (int): A paramter to determine the threshold on\n","            discarding bounding boxes based on their sizes.\n","\n","    \"\"\"\n","\n","    def __init__(self,\n","                 parent_model,\n","                 nms_thresh=0.7,\n","                 n_train_pre_nms=12000,\n","                 n_train_post_nms=2000,\n","                 n_test_pre_nms=6000,\n","                 n_test_post_nms=300,\n","                 min_size=16\n","                 ):\n","        self.parent_model = parent_model\n","        self.nms_thresh = nms_thresh\n","        self.n_train_pre_nms = n_train_pre_nms\n","        self.n_train_post_nms = n_train_post_nms\n","        self.n_test_pre_nms = n_test_pre_nms\n","        self.n_test_post_nms = n_test_post_nms\n","        self.min_size = min_size\n","\n","    def __call__(self, loc, score,\n","                 anchor, img_size, scale=1.):\n","        \"\"\"input should  be ndarray\n","        Propose RoIs.\n","\n","        Inputs :obj:`loc, score, anchor` refer to the same anchor when indexed\n","        by the same index.\n","\n","        On notations, :math:`R` is the total number of anchors. This is equal\n","        to product of the height and the width of an image and the number of\n","        anchor bases per pixel.\n","\n","        Type of the output is same as the inputs.\n","\n","        Args:\n","            loc (array): Predicted offsets and scaling to anchors.\n","                Its shape is :math:`(R, 4)`.\n","            score (array): Predicted foreground probability for anchors.\n","                Its shape is :math:`(R,)`.\n","            anchor (array): Coordinates of anchors. Its shape is\n","                :math:`(R, 4)`.\n","            img_size (tuple of ints): A tuple :obj:`height, width`,\n","                which contains image size after scaling.\n","            scale (float): The scaling factor used to scale an image after\n","                reading it from a file.\n","\n","        Returns:\n","            array:\n","            An array of coordinates of proposal boxes.\n","            Its shape is :math:`(S, 4)`. :math:`S` is less than\n","            :obj:`self.n_test_post_nms` in test time and less than\n","            :obj:`self.n_train_post_nms` in train time. :math:`S` depends on\n","            the size of the predicted bounding boxes and the number of\n","            bounding boxes discarded by NMS.\n","\n","        \"\"\"\n","        # NOTE: when test, remember\n","        # faster_rcnn.eval()\n","        # to set self.traing = False\n","        if self.parent_model.training:\n","            n_pre_nms = self.n_train_pre_nms\n","            n_post_nms = self.n_train_post_nms\n","        else:\n","            n_pre_nms = self.n_test_pre_nms\n","            n_post_nms = self.n_test_post_nms\n","\n","        # Convert anchors into proposal via bbox transformations.\n","        # roi = loc2bbox(anchor, loc)\n","        roi = loc2bbox(anchor, loc)\n","\n","        # Clip predicted boxes to image.\n","        roi[:, slice(0, 4, 2)] = np.clip(\n","            roi[:, slice(0, 4, 2)], 0, img_size[0])\n","        roi[:, slice(1, 4, 2)] = np.clip(\n","            roi[:, slice(1, 4, 2)], 0, img_size[1])\n","\n","        # Remove predicted boxes with either height or width < threshold.\n","        min_size = self.min_size * scale\n","        hs = roi[:, 2] - roi[:, 0]\n","        ws = roi[:, 3] - roi[:, 1]\n","        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n","        roi = roi[keep, :]\n","        score = score[keep]\n","\n","        # Sort all (proposal, score) pairs by score from highest to lowest.\n","        # Take top pre_nms_topN (e.g. 6000).\n","        order = score.ravel().argsort()[::-1]\n","        if n_pre_nms > 0:\n","            order = order[:n_pre_nms]\n","        roi = roi[order, :]\n","\n","        # Apply nms (e.g. threshold = 0.7).\n","        # Take after_nms_topN (e.g. 300).\n","\n","        # unNOTE: somthing is wrong here!\n","        # TODO: remove cuda.to_gpu\n","        keep = non_maximum_suppression(\n","            cp.ascontiguousarray(cp.asarray(roi)),\n","            thresh=self.nms_thresh)\n","        if n_post_nms > 0:\n","            keep = keep[:n_post_nms]\n","        roi = roi[keep]\n","        return roi\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KfwC8o4i0fSc","colab_type":"text"},"source":["## model.region_proposal_network"]},{"cell_type":"code","metadata":{"id":"nBuO01Dj0kmS","colab_type":"code","colab":{}},"source":["import numpy as np\n","from torch.nn import functional as F\n","import torch as t\n","from torch import nn\n","\n","#from model.utils.bbox_tools import generate_anchor_base\n","#from model.utils.creator_tool import ProposalCreator\n","\n","\n","class RegionProposalNetwork(nn.Module):\n","    \"\"\"Region Proposal Network introduced in Faster R-CNN.\n","\n","    This is Region Proposal Network introduced in Faster R-CNN [#]_.\n","    This takes features extracted from images and propose\n","    class agnostic bounding boxes around \"objects\".\n","\n","    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n","    Faster R-CNN: Towards Real-Time Object Detection with \\\n","    Region Proposal Networks. NIPS 2015.\n","\n","    Args:\n","        in_channels (int): The channel size of input.\n","        mid_channels (int): The channel size of the intermediate tensor.\n","        ratios (list of floats): This is ratios of width to height of\n","            the anchors.\n","        anchor_scales (list of numbers): This is areas of anchors.\n","            Those areas will be the product of the square of an element in\n","            :obj:`anchor_scales` and the original area of the reference\n","            window.\n","        feat_stride (int): Stride size after extracting features from an\n","            image.\n","        initialW (callable): Initial weight value. If :obj:`None` then this\n","            function uses Gaussian distribution scaled by 0.1 to\n","            initialize weight.\n","            May also be a callable that takes an array and edits its values.\n","        proposal_creator_params (dict): Key valued paramters for\n","            :class:`model.utils.creator_tools.ProposalCreator`.\n","\n","    .. seealso::\n","        :class:`~model.utils.creator_tools.ProposalCreator`\n","\n","    \"\"\"\n","\n","    def __init__(\n","            self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n","            anchor_scales=[8, 16, 32], feat_stride=16,\n","            proposal_creator_params=dict(),\n","    ):\n","        super(RegionProposalNetwork, self).__init__()\n","        self.anchor_base = generate_anchor_base(\n","            anchor_scales=anchor_scales, ratios=ratios)\n","        self.feat_stride = feat_stride\n","        self.proposal_layer = ProposalCreator(self, **proposal_creator_params)\n","        n_anchor = self.anchor_base.shape[0]\n","        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n","        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)\n","        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)\n","        normal_init(self.conv1, 0, 0.01)\n","        normal_init(self.score, 0, 0.01)\n","        normal_init(self.loc, 0, 0.01)\n","\n","    def forward(self, x, img_size, scale=1.):\n","        \"\"\"Forward Region Proposal Network.\n","\n","        Here are notations.\n","\n","        * :math:`N` is batch size.\n","        * :math:`C` channel size of the input.\n","        * :math:`H` and :math:`W` are height and witdh of the input feature.\n","        * :math:`A` is number of anchors assigned to each pixel.\n","\n","        Args:\n","            x (~torch.autograd.Variable): The Features extracted from images.\n","                Its shape is :math:`(N, C, H, W)`.\n","            img_size (tuple of ints): A tuple :obj:`height, width`,\n","                which contains image size after scaling.\n","            scale (float): The amount of scaling done to the input images after\n","                reading them from files.\n","\n","        Returns:\n","            (~torch.autograd.Variable, ~torch.autograd.Variable, array, array, array):\n","\n","            This is a tuple of five following values.\n","\n","            * **rpn_locs**: Predicted bounding box offsets and scales for \\\n","                anchors. Its shape is :math:`(N, H W A, 4)`.\n","            * **rpn_scores**:  Predicted foreground scores for \\\n","                anchors. Its shape is :math:`(N, H W A, 2)`.\n","            * **rois**: A bounding box array containing coordinates of \\\n","                proposal boxes.  This is a concatenation of bounding box \\\n","                arrays from multiple images in the batch. \\\n","                Its shape is :math:`(R', 4)`. Given :math:`R_i` predicted \\\n","                bounding boxes from the :math:`i` th image, \\\n","                :math:`R' = \\\\sum _{i=1} ^ N R_i`.\n","            * **roi_indices**: An array containing indices of images to \\\n","                which RoIs correspond to. Its shape is :math:`(R',)`.\n","            * **anchor**: Coordinates of enumerated shifted anchors. \\\n","                Its shape is :math:`(H W A, 4)`.\n","\n","        \"\"\"\n","        n, _, hh, ww = x.shape\n","        anchor = _enumerate_shifted_anchor(\n","            np.array(self.anchor_base),\n","            self.feat_stride, hh, ww)\n","\n","        n_anchor = anchor.shape[0] // (hh * ww)\n","        h = F.relu(self.conv1(x))\n","\n","        rpn_locs = self.loc(h)\n","        # UNNOTE: check whether need contiguous\n","        # A: Yes\n","        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4)\n","        rpn_scores = self.score(h)\n","        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous()\n","        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4)\n","        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous()\n","        rpn_fg_scores = rpn_fg_scores.view(n, -1)\n","        rpn_scores = rpn_scores.view(n, -1, 2)\n","\n","        rois = list()\n","        roi_indices = list()\n","        for i in range(n):\n","            roi = self.proposal_layer(\n","                rpn_locs[i].cpu().data.numpy(),\n","                rpn_fg_scores[i].cpu().data.numpy(),\n","                anchor, img_size,\n","                scale=scale)\n","            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n","            rois.append(roi)\n","            roi_indices.append(batch_index)\n","\n","        rois = np.concatenate(rois, axis=0)\n","        roi_indices = np.concatenate(roi_indices, axis=0)\n","        return rpn_locs, rpn_scores, rois, roi_indices, anchor\n","\n","\n","def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n","    # Enumerate all shifted anchors:\n","    #\n","    # add A anchors (1, A, 4) to\n","    # cell K shifts (K, 1, 4) to get\n","    # shift anchors (K, A, 4)\n","    # reshape to (K*A, 4) shifted anchors\n","    # return (K*A, 4)\n","\n","    # !TODO: add support for torch.CudaTensor\n","    # xp = cuda.get_array_module(anchor_base)\n","    # it seems that it can't be boosed using GPU\n","    import numpy as xp\n","    shift_y = xp.arange(0, height * feat_stride, feat_stride)\n","    shift_x = xp.arange(0, width * feat_stride, feat_stride)\n","    shift_x, shift_y = xp.meshgrid(shift_x, shift_y)\n","    shift = xp.stack((shift_y.ravel(), shift_x.ravel(),\n","                      shift_y.ravel(), shift_x.ravel()), axis=1)\n","\n","    A = anchor_base.shape[0]\n","    K = shift.shape[0]\n","    anchor = anchor_base.reshape((1, A, 4)) + \\\n","             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n","    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n","    return anchor\n","\n","\n","def _enumerate_shifted_anchor_torch(anchor_base, feat_stride, height, width):\n","    # Enumerate all shifted anchors:\n","    #\n","    # add A anchors (1, A, 4) to\n","    # cell K shifts (K, 1, 4) to get\n","    # shift anchors (K, A, 4)\n","    # reshape to (K*A, 4) shifted anchors\n","    # return (K*A, 4)\n","\n","    # !TODO: add support for torch.CudaTensor\n","    # xp = cuda.get_array_module(anchor_base)\n","    import torch as t\n","    shift_y = t.arange(0, height * feat_stride, feat_stride)\n","    shift_x = t.arange(0, width * feat_stride, feat_stride)\n","    shift_x, shift_y = xp.meshgrid(shift_x, shift_y)\n","    shift = xp.stack((shift_y.ravel(), shift_x.ravel(),\n","                      shift_y.ravel(), shift_x.ravel()), axis=1)\n","\n","    A = anchor_base.shape[0]\n","    K = shift.shape[0]\n","    anchor = anchor_base.reshape((1, A, 4)) + \\\n","             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n","    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n","    return anchor\n","\n","\n","def normal_init(m, mean, stddev, truncated=False):\n","    \"\"\"\n","    weight initalizer: truncated normal and random normal.\n","    \"\"\"\n","    # x is a parameter\n","    if truncated:\n","        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n","    else:\n","        m.weight.data.normal_(mean, stddev)\n","        m.bias.data.zero_()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SvmctCan2T3r","colab_type":"text"},"source":["## utils.array_tool"]},{"cell_type":"code","metadata":{"id":"TTyk3lAA2U3E","colab_type":"code","colab":{}},"source":["\"\"\"\n","tools to convert specified type\n","\"\"\"\n","import torch as t\n","import numpy as np\n","\n","\n","def tonumpy(data):\n","    if isinstance(data, np.ndarray):\n","        return data\n","    if isinstance(data, t.Tensor):\n","        return data.detach().cpu().numpy()\n","\n","\n","def totensor(data, cuda=True):\n","    if isinstance(data, np.ndarray):\n","        tensor = t.from_numpy(data)\n","    if isinstance(data, t.Tensor):\n","        tensor = data.detach()\n","    if cuda:\n","        tensor = tensor.cuda()\n","    return tensor\n","\n","\n","def scalar(data):\n","    if isinstance(data, np.ndarray):\n","        return data.reshape(1)[0]\n","    if isinstance(data, t.Tensor):\n","        return data.item()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_mf4J6Hp0ora","colab_type":"text"},"source":["## model.faster_rcnn"]},{"cell_type":"code","metadata":{"id":"FuKgvfk61-B5","colab_type":"code","colab":{}},"source":["#from __future__ import  absolute_import\n","#from __future__ import division\n","import torch as t\n","import numpy as np\n","import cupy as cp\n","#from utils import array_tool as at\n","#from model.utils.bbox_tools import loc2bbox\n","#from model.utils.nms import non_maximum_suppression\n","\n","from torch import nn\n","#from data.dataset import preprocess\n","from torch.nn import functional as F\n","#from utils.config import opt\n","\n","\n","def nograd(f):\n","    def new_f(*args,**kwargs):\n","        with t.no_grad():\n","           return f(*args,**kwargs)\n","    return new_f\n","\n","class FasterRCNN(nn.Module):\n","    \"\"\"Base class for Faster R-CNN.\n","\n","    This is a base class for Faster R-CNN links supporting object detection\n","    API [#]_. The following three stages constitute Faster R-CNN.\n","\n","    1. **Feature extraction**: Images are taken and their \\\n","        feature maps are calculated.\n","    2. **Region Proposal Networks**: Given the feature maps calculated in \\\n","        the previous stage, produce set of RoIs around objects.\n","    3. **Localization and Classification Heads**: Using feature maps that \\\n","        belong to the proposed RoIs, classify the categories of the objects \\\n","        in the RoIs and improve localizations.\n","\n","    Each stage is carried out by one of the callable\n","    :class:`torch.nn.Module` objects :obj:`feature`, :obj:`rpn` and :obj:`head`.\n","\n","    There are two functions :meth:`predict` and :meth:`__call__` to conduct\n","    object detection.\n","    :meth:`predict` takes images and returns bounding boxes that are converted\n","    to image coordinates. This will be useful for a scenario when\n","    Faster R-CNN is treated as a black box function, for instance.\n","    :meth:`__call__` is provided for a scnerario when intermediate outputs\n","    are needed, for instance, for training and debugging.\n","\n","    Links that support obejct detection API have method :meth:`predict` with\n","    the same interface. Please refer to :meth:`predict` for\n","    further details.\n","\n","    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n","    Faster R-CNN: Towards Real-Time Object Detection with \\\n","    Region Proposal Networks. NIPS 2015.\n","\n","    Args:\n","        extractor (nn.Module): A module that takes a BCHW image\n","            array and returns feature maps.\n","        rpn (nn.Module): A module that has the same interface as\n","            :class:`model.region_proposal_network.RegionProposalNetwork`.\n","            Please refer to the documentation found there.\n","        head (nn.Module): A module that takes\n","            a BCHW variable, RoIs and batch indices for RoIs. This returns class\n","            dependent localization paramters and class scores.\n","        loc_normalize_mean (tuple of four floats): Mean values of\n","            localization estimates.\n","        loc_normalize_std (tupler of four floats): Standard deviation\n","            of localization estimates.\n","\n","    \"\"\"\n","\n","    def __init__(self, extractor, rpn, head,\n","                loc_normalize_mean = (0., 0., 0., 0.),\n","                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)\n","    ):\n","        super(FasterRCNN, self).__init__()\n","        self.extractor = extractor\n","        self.rpn = rpn\n","        self.head = head\n","\n","        # mean and std\n","        self.loc_normalize_mean = loc_normalize_mean\n","        self.loc_normalize_std = loc_normalize_std\n","        self.use_preset('evaluate')\n","\n","    @property\n","    def n_class(self):\n","        # Total number of classes including the background.\n","        return self.head.n_class\n","\n","    def forward(self, x, scale=1.):\n","        \"\"\"Forward Faster R-CNN.\n","\n","        Scaling paramter :obj:`scale` is used by RPN to determine the\n","        threshold to select small objects, which are going to be\n","        rejected irrespective of their confidence scores.\n","\n","        Here are notations used.\n","\n","        * :math:`N` is the number of batch size\n","        * :math:`R'` is the total number of RoIs produced across batches. \\\n","            Given :math:`R_i` proposed RoIs from the :math:`i` th image, \\\n","            :math:`R' = \\\\sum _{i=1} ^ N R_i`.\n","        * :math:`L` is the number of classes excluding the background.\n","\n","        Classes are ordered by the background, the first class, ..., and\n","        the :math:`L` th class.\n","\n","        Args:\n","            x (autograd.Variable): 4D image variable.\n","            scale (float): Amount of scaling applied to the raw image\n","                during preprocessing.\n","\n","        Returns:\n","            Variable, Variable, array, array:\n","            Returns tuple of four values listed below.\n","\n","            * **roi_cls_locs**: Offsets and scalings for the proposed RoIs. \\\n","                Its shape is :math:`(R', (L + 1) \\\\times 4)`.\n","            * **roi_scores**: Class predictions for the proposed RoIs. \\\n","                Its shape is :math:`(R', L + 1)`.\n","            * **rois**: RoIs proposed by RPN. Its shape is \\\n","                :math:`(R', 4)`.\n","            * **roi_indices**: Batch indices of RoIs. Its shape is \\\n","                :math:`(R',)`.\n","\n","        \"\"\"\n","        img_size = x.shape[2:]\n","\n","        h = self.extractor(x)\n","        rpn_locs, rpn_scores, rois, roi_indices, anchor = \\\n","            self.rpn(h, img_size, scale)\n","        roi_cls_locs, roi_scores = self.head(\n","            h, rois, roi_indices)\n","        return roi_cls_locs, roi_scores, rois, roi_indices\n","\n","    def use_preset(self, preset):\n","        \"\"\"Use the given preset during prediction.\n","\n","        This method changes values of :obj:`self.nms_thresh` and\n","        :obj:`self.score_thresh`. These values are a threshold value\n","        used for non maximum suppression and a threshold value\n","        to discard low confidence proposals in :meth:`predict`,\n","        respectively.\n","\n","        If the attributes need to be changed to something\n","        other than the values provided in the presets, please modify\n","        them by directly accessing the public attributes.\n","\n","        Args:\n","            preset ({'visualize', 'evaluate'): A string to determine the\n","                preset to use.\n","\n","        \"\"\"\n","        if preset == 'visualize':\n","            self.nms_thresh = 0.3\n","            self.score_thresh = 0.7\n","        elif preset == 'evaluate':\n","            self.nms_thresh = 0.3\n","            self.score_thresh = 0.05\n","        else:\n","            raise ValueError('preset must be visualize or evaluate')\n","\n","    def _suppress(self, raw_cls_bbox, raw_prob):\n","        bbox = list()\n","        label = list()\n","        score = list()\n","        # skip cls_id = 0 because it is the background class\n","        for l in range(1, self.n_class):\n","            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n","            prob_l = raw_prob[:, l]\n","            mask = prob_l > self.score_thresh\n","            cls_bbox_l = cls_bbox_l[mask]\n","            prob_l = prob_l[mask]\n","            keep = non_maximum_suppression(\n","                cp.array(cls_bbox_l), self.nms_thresh, prob_l)\n","            keep = cp.asnumpy(keep)\n","            bbox.append(cls_bbox_l[keep])\n","            # The labels are in [0, self.n_class - 2].\n","            label.append((l - 1) * np.ones((len(keep),)))\n","            score.append(prob_l[keep])\n","        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n","        label = np.concatenate(label, axis=0).astype(np.int32)\n","        score = np.concatenate(score, axis=0).astype(np.float32)\n","        return bbox, label, score\n","\n","    @nograd\n","    def predict(self, imgs,sizes=None,visualize=False):\n","        \"\"\"Detect objects from images.\n","\n","        This method predicts objects for each image.\n","\n","        Args:\n","            imgs (iterable of numpy.ndarray): Arrays holding images.\n","                All images are in CHW and RGB format\n","                and the range of their value is :math:`[0, 255]`.\n","\n","        Returns:\n","           tuple of lists:\n","           This method returns a tuple of three lists,\n","           :obj:`(bboxes, labels, scores)`.\n","\n","           * **bboxes**: A list of float arrays of shape :math:`(R, 4)`, \\\n","               where :math:`R` is the number of bounding boxes in a image. \\\n","               Each bouding box is organized by \\\n","               :math:`(y_{min}, x_{min}, y_{max}, x_{max})` \\\n","               in the second axis.\n","           * **labels** : A list of integer arrays of shape :math:`(R,)`. \\\n","               Each value indicates the class of the bounding box. \\\n","               Values are in range :math:`[0, L - 1]`, where :math:`L` is the \\\n","               number of the foreground classes.\n","           * **scores** : A list of float arrays of shape :math:`(R,)`. \\\n","               Each value indicates how confident the prediction is.\n","\n","        \"\"\"\n","        self.eval()\n","        if visualize:\n","            self.use_preset('visualize')\n","            prepared_imgs = list()\n","            sizes = list()\n","            for img in imgs:\n","                size = img.shape[1:]\n","                img = preprocess(tonumpy(img))\n","                prepared_imgs.append(img)\n","                sizes.append(size)\n","        else:\n","             prepared_imgs = imgs \n","        bboxes = list()\n","        labels = list()\n","        scores = list()\n","        for img, size in zip(prepared_imgs, sizes):\n","            img = totensor(img[None]).float()\n","            scale = img.shape[3] / size[1]\n","            roi_cls_loc, roi_scores, rois, _ = self(img, scale=scale)\n","            # We are assuming that batch size is 1.\n","            roi_score = roi_scores.data\n","            roi_cls_loc = roi_cls_loc.data\n","            roi = totensor(rois) / scale\n","\n","            # Convert predictions to bounding boxes in image coordinates.\n","            # Bounding boxes are scaled to the scale of the input images.\n","            mean = t.Tensor(self.loc_normalize_mean).cuda(). \\\n","                repeat(self.n_class)[None]\n","            std = t.Tensor(self.loc_normalize_std).cuda(). \\\n","                repeat(self.n_class)[None]\n","\n","            roi_cls_loc = (roi_cls_loc * std + mean)\n","            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n","            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n","            cls_bbox = loc2bbox(tonumpy(roi).reshape((-1, 4)),\n","                                tonumpy(roi_cls_loc).reshape((-1, 4)))\n","            cls_bbox = totensor(cls_bbox)\n","            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n","            # clip bounding box\n","            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n","            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n","\n","            prob = tonumpy(F.softmax(totensor(roi_score), dim=1))\n","\n","            raw_cls_bbox = tonumpy(cls_bbox)\n","            raw_prob = tonumpy(prob)\n","\n","            bbox, label, score = self._suppress(raw_cls_bbox, raw_prob)\n","            bboxes.append(bbox)\n","            labels.append(label)\n","            scores.append(score)\n","\n","        self.use_preset('evaluate')\n","        self.train()\n","        return bboxes, labels, scores\n","\n","    def get_optimizer(self):\n","        \"\"\"\n","        return optimizer, It could be overwriten if you want to specify \n","        special optimizer\n","        \"\"\"\n","        lr = opt.lr\n","        params = []\n","        for key, value in dict(self.named_parameters()).items():\n","            if value.requires_grad:\n","                if 'bias' in key:\n","                    params += [{'params': [value], 'lr': lr * 2, 'weight_decay': 0}]\n","                else:\n","                    params += [{'params': [value], 'lr': lr, 'weight_decay': opt.weight_decay}]\n","        if opt.use_adam:\n","            self.optimizer = t.optim.Adam(params)\n","        else:\n","            self.optimizer = t.optim.SGD(params, momentum=0.9)\n","        return self.optimizer\n","\n","    def scale_lr(self, decay=0.1):\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] *= decay\n","        return self.optimizer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xAnPRfHc3_OJ","colab_type":"text"},"source":["## model.utils.roi_cupy"]},{"cell_type":"code","metadata":{"id":"uvsUFBNM4Ciy","colab_type":"code","colab":{}},"source":["kernel_forward = '''\n","    extern \"C\"\n","    __global__ void roi_forward(const float* const bottom_data,const float* const bottom_rois,\n","                float* top_data, int* argmax_data,\n","                const double spatial_scale,const int channels,const int height, \n","                const int width, const int pooled_height, \n","                const int pooled_width,const int NN\n","    ){\n","        \n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if(idx>=NN)\n","        return;\n","    const int pw = idx % pooled_width;\n","    const int ph = (idx / pooled_width) % pooled_height;\n","    const int c = (idx / pooled_width / pooled_height) % channels;\n","    int num = idx / pooled_width / pooled_height / channels;\n","    const int roi_batch_ind = bottom_rois[num * 5 + 0];\n","    const int roi_start_w = round(bottom_rois[num * 5 + 1] * spatial_scale);\n","    const int roi_start_h = round(bottom_rois[num * 5 + 2] * spatial_scale);\n","    const int roi_end_w = round(bottom_rois[num * 5 + 3] * spatial_scale);\n","    const int roi_end_h = round(bottom_rois[num * 5 + 4] * spatial_scale);\n","    // Force malformed ROIs to be 1x1\n","    const int roi_width = max(roi_end_w - roi_start_w + 1, 1);\n","    const int roi_height = max(roi_end_h - roi_start_h + 1, 1);\n","    const float bin_size_h = static_cast<float>(roi_height)\n","                    / static_cast<float>(pooled_height);\n","    const float bin_size_w = static_cast<float>(roi_width)\n","                    / static_cast<float>(pooled_width);\n","\n","    int hstart = static_cast<int>(floor(static_cast<float>(ph)\n","                                    * bin_size_h));\n","        int wstart = static_cast<int>(floor(static_cast<float>(pw)\n","                                    * bin_size_w));\n","    int hend = static_cast<int>(ceil(static_cast<float>(ph + 1)\n","                                * bin_size_h));\n","        int wend = static_cast<int>(ceil(static_cast<float>(pw + 1)\n","                                * bin_size_w));\n","\n","    // Add roi offsets and clip to input boundaries\n","    hstart = min(max(hstart + roi_start_h, 0), height);\n","    hend = min(max(hend + roi_start_h, 0), height);\n","    wstart = min(max(wstart + roi_start_w, 0), width);\n","    wend = min(max(wend + roi_start_w, 0), width);\n","    bool is_empty = (hend <= hstart) || (wend <= wstart);\n","\n","    // Define an empty pooling region to be zero\n","    float maxval = is_empty ? 0 : -1E+37;\n","    // If nothing is pooled, argmax=-1 causes nothing to be backprop'd\n","    int maxidx = -1;\n","    const int data_offset = (roi_batch_ind * channels + c) * height * width;\n","    for (int h = hstart; h < hend; ++h) {\n","        for (int w = wstart; w < wend; ++w) {\n","            int bottom_index = h * width + w;\n","            if (bottom_data[data_offset + bottom_index] > maxval) {\n","                maxval = bottom_data[data_offset + bottom_index];\n","                maxidx = bottom_index;\n","            }\n","        }\n","    }\n","    top_data[idx]=maxval;\n","    argmax_data[idx]=maxidx;\n","    }\n","'''\n","kernel_backward = '''\n","    extern \"C\"\n","    __global__ void roi_backward(const float* const top_diff,\n","         const int* const argmax_data,const float* const bottom_rois,\n","         float* bottom_diff, const int num_rois,\n","         const double spatial_scale, int channels,\n","         int height, int width, int pooled_height,\n","          int pooled_width,const int NN)\n","    {\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    ////Importtan >= instead of >\n","    if(idx>=NN)\n","        return;\n","    int w = idx % width;\n","    int h = (idx / width) % height;\n","    int c = (idx/ (width * height)) % channels;\n","    int num = idx / (width * height * channels);\n","\n","    float gradient = 0;\n","    // Accumulate gradient over all ROIs that pooled this element\n","    for (int roi_n = 0; roi_n < num_rois; ++roi_n) {\n","        // Skip if ROI's batch index doesn't match num\n","        if (num != static_cast<int>(bottom_rois[roi_n * 5])) {\n","            continue;\n","        }\n","\n","        int roi_start_w = round(bottom_rois[roi_n * 5 + 1]\n","                                * spatial_scale);\n","        int roi_start_h = round(bottom_rois[roi_n * 5 + 2]\n","                                * spatial_scale);\n","        int roi_end_w = round(bottom_rois[roi_n * 5 + 3]\n","                                * spatial_scale);\n","        int roi_end_h = round(bottom_rois[roi_n * 5 + 4]\n","                                * spatial_scale);\n","\n","        // Skip if ROI doesn't include (h, w)\n","        const bool in_roi = (w >= roi_start_w && w <= roi_end_w &&\n","                                h >= roi_start_h && h <= roi_end_h);\n","        if (!in_roi) {\n","            continue;\n","        }\n","\n","        int offset = (roi_n * channels + c) * pooled_height\n","                        * pooled_width;\n","\n","        // Compute feasible set of pooled units that could have pooled\n","        // this bottom unit\n","\n","        // Force malformed ROIs to be 1x1\n","        int roi_width = max(roi_end_w - roi_start_w + 1, 1);\n","        int roi_height = max(roi_end_h - roi_start_h + 1, 1);\n","\n","        float bin_size_h = static_cast<float>(roi_height)\n","                        / static_cast<float>(pooled_height);\n","        float bin_size_w = static_cast<float>(roi_width)\n","                        / static_cast<float>(pooled_width);\n","\n","        int phstart = floor(static_cast<float>(h - roi_start_h)\n","                            / bin_size_h);\n","        int phend = ceil(static_cast<float>(h - roi_start_h + 1)\n","                            / bin_size_h);\n","        int pwstart = floor(static_cast<float>(w - roi_start_w)\n","                            / bin_size_w);\n","        int pwend = ceil(static_cast<float>(w - roi_start_w + 1)\n","                            / bin_size_w);\n","\n","        phstart = min(max(phstart, 0), pooled_height);\n","        phend = min(max(phend, 0), pooled_height);\n","        pwstart = min(max(pwstart, 0), pooled_width);\n","        pwend = min(max(pwend, 0), pooled_width);\n","        for (int ph = phstart; ph < phend; ++ph) {\n","            for (int pw = pwstart; pw < pwend; ++pw) {\n","                int index_ = ph * pooled_width + pw + offset;\n","                if (argmax_data[index_] == (h * width + w)) {\n","                    gradient += top_diff[index_];\n","                }\n","            }\n","        }\n","    }\n","    bottom_diff[idx] = gradient;\n","    }\n","'''\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E2fc8QBZ145x","colab_type":"text"},"source":["## model.roi_module"]},{"cell_type":"code","metadata":{"id":"5KHuV1Wi1_c7","colab_type":"code","colab":{}},"source":["from collections import namedtuple\n","from string import Template\n","\n","import cupy, torch\n","import cupy as cp\n","import torch as t\n","from torch.autograd import Function\n","\n","#from model.utils.roi_cupy import kernel_backward, kernel_forward\n","\n","Stream = namedtuple('Stream', ['ptr'])\n","\n","\n","@cupy.util.memoize(for_each_device=True)\n","def load_kernel(kernel_name, code, **kwargs):\n","    cp.cuda.runtime.free(0)\n","    code = Template(code).substitute(**kwargs)\n","    kernel_code = cupy.cuda.compile_with_cache(code)\n","    return kernel_code.get_function(kernel_name)\n","\n","\n","CUDA_NUM_THREADS = 1024\n","\n","\n","def GET_BLOCKS(N, K=CUDA_NUM_THREADS):\n","    return (N + K - 1) // K\n","\n","\n","class RoI(Function):\n","    def __init__(self, outh, outw, spatial_scale):\n","        self.forward_fn = load_kernel('roi_forward', kernel_forward)\n","        self.backward_fn = load_kernel('roi_backward', kernel_backward)\n","        self.outh, self.outw, self.spatial_scale = outh, outw, spatial_scale\n","\n","    def forward(self, x, rois):\n","        # NOTE: MAKE SURE input is contiguous too\n","        x = x.contiguous()\n","        rois = rois.contiguous()\n","        self.in_size = B, C, H, W = x.size()\n","        self.N = N = rois.size(0)\n","        output = t.zeros(N, C, self.outh, self.outw).cuda()\n","        self.argmax_data = t.zeros(N, C, self.outh, self.outw).int().cuda()\n","        self.rois = rois\n","        args = [x.data_ptr(), rois.data_ptr(),\n","                output.data_ptr(),\n","                self.argmax_data.data_ptr(),\n","                self.spatial_scale, C, H, W,\n","                self.outh, self.outw,\n","                output.numel()]\n","        stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n","        self.forward_fn(args=args,\n","                        block=(CUDA_NUM_THREADS, 1, 1),\n","                        grid=(GET_BLOCKS(output.numel()), 1, 1),\n","                        stream=stream)\n","        return output\n","\n","    def backward(self, grad_output):\n","        ##NOTE: IMPORTANT CONTIGUOUS\n","        # TODO: input\n","        grad_output = grad_output.contiguous()\n","        B, C, H, W = self.in_size\n","        grad_input = t.zeros(self.in_size).cuda()\n","        stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n","        args = [grad_output.data_ptr(),\n","                self.argmax_data.data_ptr(),\n","                self.rois.data_ptr(),\n","                grad_input.data_ptr(),\n","                self.N, self.spatial_scale, C, H, W, self.outh, self.outw,\n","                grad_input.numel()]\n","        self.backward_fn(args=args,\n","                         block=(CUDA_NUM_THREADS, 1, 1),\n","                         grid=(GET_BLOCKS(grad_input.numel()), 1, 1),\n","                         stream=stream\n","                         )\n","        return grad_input, None\n","\n","\n","class RoIPooling2D(t.nn.Module):\n","\n","    def __init__(self, outh, outw, spatial_scale):\n","        super(RoIPooling2D, self).__init__()\n","        self.RoI = RoI(outh, outw, spatial_scale)\n","\n","    def forward(self, x, rois):\n","        return self.RoI(x, rois)\n","\n","\n","def test_roi_module():\n","    ## fake data###\n","    B, N, C, H, W, PH, PW = 2, 8, 4, 32, 32, 7, 7\n","\n","    bottom_data = t.randn(B, C, H, W).cuda()\n","    bottom_rois = t.randn(N, 5)\n","    bottom_rois[:int(N / 2), 0] = 0\n","    bottom_rois[int(N / 2):, 0] = 1\n","    bottom_rois[:, 1:] = (t.rand(N, 4) * 100).float()\n","    bottom_rois = bottom_rois.cuda()\n","    spatial_scale = 1. / 16\n","    outh, outw = PH, PW\n","\n","    # pytorch version\n","    module = RoIPooling2D(outh, outw, spatial_scale)\n","    x = bottom_data.requires_grad_()\n","    rois = bottom_rois.detach()\n","\n","    output = module(x, rois)\n","    output.sum().backward()\n","\n","    def t2c(variable):\n","        npa = variable.data.cpu().numpy()\n","        return cp.array(npa)\n","\n","    def test_eq(variable, array, info):\n","        cc = cp.asnumpy(array)\n","        neq = (cc != variable.data.cpu().numpy())\n","        assert neq.sum() == 0, 'test failed: %s' % info\n","\n","    # chainer version,if you're going to run this\n","    # pip install chainer \n","    import chainer.functions as F\n","    from chainer import Variable\n","    x_cn = Variable(t2c(x))\n","\n","    o_cn = F.roi_pooling_2d(x_cn, t2c(rois), outh, outw, spatial_scale)\n","    test_eq(output, o_cn.array, 'forward')\n","    F.sum(o_cn).backward()\n","    test_eq(x.grad, x_cn.grad, 'backward')\n","    print('test pass')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fXVW39bC0GOi","colab_type":"text"},"source":["## model.faster_rcnn_vgg16"]},{"cell_type":"code","metadata":{"id":"mKhCyMMryd6S","colab_type":"code","colab":{}},"source":["#from __future__ import  absolute_import\n","import torch as t\n","from torch import nn\n","from torchvision.models import vgg16\n","#from model.region_proposal_network import RegionProposalNetwork\n","#from model.faster_rcnn import FasterRCNN\n","#from model.roi_module import RoIPooling2D\n","#from utils import array_tool as at\n","#from utils.config import opt\n","\n","\n","def decom_vgg16():\n","    # the 30th layer of features is relu of conv5_3\n","    if opt.caffe_pretrain:\n","        model = vgg16(pretrained=False)\n","        if not opt.load_path:\n","            model.load_state_dict(t.load(opt.caffe_pretrain_path))\n","    else:\n","        model = vgg16(not opt.load_path)\n","\n","    features = list(model.features)[:30]\n","    classifier = model.classifier\n","\n","    classifier = list(classifier)\n","    del classifier[6]\n","    if not opt.use_drop:\n","        del classifier[5]\n","        del classifier[2]\n","    classifier = nn.Sequential(*classifier)\n","\n","    # freeze top4 conv                                                                 # !\n","#     for layer in features[:10]:\n","#         for p in layer.parameters():\n","#             p.requires_grad = False\n","\n","    return nn.Sequential(*features), classifier\n","\n","\n","class FasterRCNNVGG16(FasterRCNN):\n","    \"\"\"Faster R-CNN based on VGG-16.\n","    For descriptions on the interface of this model, please refer to\n","    :class:`model.faster_rcnn.FasterRCNN`.\n","\n","    Args:\n","        n_fg_class (int): The number of classes excluding the background.\n","        ratios (list of floats): This is ratios of width to height of\n","            the anchors.\n","        anchor_scales (list of numbers): This is areas of anchors.\n","            Those areas will be the product of the square of an element in\n","            :obj:`anchor_scales` and the original area of the reference\n","            window.\n","\n","    \"\"\"\n","\n","    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n","\n","    def __init__(self,\n","                 n_fg_class=20,\n","                 ratios=[0.5, 1, 2],\n","                 anchor_scales=[8, 16, 32]\n","                 ):\n","                 \n","        extractor, classifier = decom_vgg16()\n","\n","        rpn = RegionProposalNetwork(\n","            512, 512,\n","            ratios=ratios,\n","            anchor_scales=anchor_scales,\n","            feat_stride=self.feat_stride,\n","        )\n","\n","        head = VGG16RoIHead(\n","            n_class=n_fg_class + 1,\n","            roi_size=7,\n","            spatial_scale=(1. / self.feat_stride),\n","            classifier=classifier\n","        )\n","\n","        super(FasterRCNNVGG16, self).__init__(\n","            extractor,\n","            rpn,\n","            head,\n","        )\n","\n","\n","class VGG16RoIHead(nn.Module):\n","    \"\"\"Faster R-CNN Head for VGG-16 based implementation.\n","    This class is used as a head for Faster R-CNN.\n","    This outputs class-wise localizations and classification based on feature\n","    maps in the given RoIs.\n","    \n","    Args:\n","        n_class (int): The number of classes possibly including the background.\n","        roi_size (int): Height and width of the feature maps after RoI-pooling.\n","        spatial_scale (float): Scale of the roi is resized.\n","        classifier (nn.Module): Two layer Linear ported from vgg16\n","\n","    \"\"\"\n","\n","    def __init__(self, n_class, roi_size, spatial_scale,\n","                 classifier):\n","        # n_class includes the background\n","        super(VGG16RoIHead, self).__init__()\n","\n","        self.classifier = classifier\n","        self.cls_loc = nn.Linear(4096, n_class * 4)\n","        self.score = nn.Linear(4096, n_class)\n","\n","        normal_init(self.cls_loc, 0, 0.001)\n","        normal_init(self.score, 0, 0.01)\n","\n","        self.n_class = n_class\n","        self.roi_size = roi_size\n","        self.spatial_scale = spatial_scale\n","        self.roi = RoIPooling2D(self.roi_size, self.roi_size, self.spatial_scale)\n","\n","    def forward(self, x, rois, roi_indices):\n","        \"\"\"Forward the chain.\n","\n","        We assume that there are :math:`N` batches.\n","\n","        Args:\n","            x (Variable): 4D image variable.\n","            rois (Tensor): A bounding box array containing coordinates of\n","                proposal boxes.  This is a concatenation of bounding box\n","                arrays from multiple images in the batch.\n","                Its shape is :math:`(R', 4)`. Given :math:`R_i` proposed\n","                RoIs from the :math:`i` th image,\n","                :math:`R' = \\\\sum _{i=1} ^ N R_i`.\n","            roi_indices (Tensor): An array containing indices of images to\n","                which bounding boxes correspond to. Its shape is :math:`(R',)`.\n","\n","        \"\"\"\n","        # in case roi_indices is  ndarray\n","        #roi_indices = t.from_numpy(roi_indices).float().cuda()                       #\n","        #rois = t.from_numpy(rois).float().cuda()                                     #\n","        roi_indices = totensor(roi_indices).float()\n","        rois = totensor(rois).float()\n","        indices_and_rois = t.cat([roi_indices[:, None], rois], dim=1)\n","        # NOTE: important: yx->xy\n","        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n","        indices_and_rois =  xy_indices_and_rois.contiguous()\n","\n","        pool = self.roi(x, indices_and_rois)\n","        pool = pool.view(pool.size(0), -1)\n","        fc7 = self.classifier(pool)\n","        roi_cls_locs = self.cls_loc(fc7)\n","        roi_scores = self.score(fc7)\n","        return roi_cls_locs, roi_scores\n","\n","\n","def normal_init(m, mean, stddev, truncated=False):\n","    \"\"\"\n","    weight initalizer: truncated normal and random normal.\n","    \"\"\"\n","    # x is a parameter\n","    if truncated:\n","        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n","    else:\n","        m.weight.data.normal_(mean, stddev)\n","        m.bias.data.zero_()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dE-pkRLv3DLS","colab_type":"text"},"source":["## Test Model"]},{"cell_type":"code","metadata":{"id":"kdF5RoxI3ExN","colab_type":"code","outputId":"ac368c4a-3ed7-46e9-8a55-722c66776d38","executionInfo":{"status":"ok","timestamp":1559020913765,"user_tz":-480,"elapsed":14959,"user":{"displayName":"张耿","photoUrl":"","userId":"17787706570393703878"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","faster_rcnn = FasterRCNNVGG16()\n","\n","faster_rcnn.to(device)\n","img = img.to(device)\n","\n","roi_cls_locs, roi_scores, rois, roi_indices = faster_rcnn(img)\n","print(roi_cls_locs.shape, roi_scores.shape,\n","      rois.shape, roi_indices.shape)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["cuda\n","torch.Size([2000, 84]) torch.Size([2000, 21]) (2000, 4) (2000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c3GzDJMM48H4","colab_type":"text"},"source":["# Train"]},{"cell_type":"markdown","metadata":{"id":"MvyBhgCvSZTN","colab_type":"text"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"6UxKwZNO4nl7","colab_type":"code","colab":{}},"source":["import os\n","from collections import namedtuple\n","import time\n","from torch.nn import functional as F\n","#from model.utils.creator_tool import AnchorTargetCreator, ProposalTargetCreator\n","\n","from torch import nn\n","import torch as t\n","#from utils import array_tool as at\n","#from utils.vis_tool import Visualizer\n","\n","#from utils.config import opt\n","#from torchnet.meter import ConfusionMeter, AverageValueMeter\n","\n","LossTuple = namedtuple('LossTuple',\n","                       ['rpn_loc_loss',\n","                        'rpn_cls_loss',\n","                        'roi_loc_loss',\n","                        'roi_cls_loss',\n","                        'total_loss'\n","                        ])\n","\n","\n","class FasterRCNNTrainer(nn.Module):\n","\n","    def __init__(self, faster_rcnn):\n","        super(FasterRCNNTrainer, self).__init__()\n","\n","        self.faster_rcnn = faster_rcnn\n","        self.rpn_sigma = opt.rpn_sigma\n","        self.roi_sigma = opt.roi_sigma\n","\n","        # target creator create gt_bbox gt_label etc as training targets. \n","        self.anchor_target_creator = AnchorTargetCreator()\n","        self.proposal_target_creator = ProposalTargetCreator()\n","\n","        self.loc_normalize_mean = faster_rcnn.loc_normalize_mean\n","        self.loc_normalize_std = faster_rcnn.loc_normalize_std\n","\n","        self.optimizer = self.faster_rcnn.get_optimizer()\n","\n","\n","    def forward(self, imgs, bboxes, labels, scale):\n","        n = bboxes.shape[0]\n","        if n != 1:\n","            raise ValueError('Currently only batch size 1 is supported.')\n","\n","        _, _, H, W = imgs.shape\n","        img_size = (H, W)\n","\n","        features = self.faster_rcnn.extractor(imgs)\n","\n","        rpn_locs, rpn_scores, rois, roi_indices, anchor = \\\n","            self.faster_rcnn.rpn(features, img_size, scale)\n","\n","        # Since batch size is one, convert variables to singular form\n","        bbox = bboxes[0]\n","        label = labels[0]\n","        rpn_score = rpn_scores[0]\n","        rpn_loc = rpn_locs[0]\n","        roi = rois\n","\n","        # Sample RoIs and forward\n","        # it's fine to break the computation graph of rois, \n","        # consider them as constant input\n","        sample_roi, gt_roi_loc, gt_roi_label = self.proposal_target_creator(\n","            roi,\n","            tonumpy(bbox),\n","            tonumpy(label),\n","            self.loc_normalize_mean,\n","            self.loc_normalize_std)\n","        # NOTE it's all zero because now it only support for batch=1 now\n","        sample_roi_index = t.zeros(len(sample_roi))\n","        roi_cls_loc, roi_score = self.faster_rcnn.head(\n","            features,\n","            sample_roi,\n","            sample_roi_index)\n","\n","        # ------------------ RPN losses -------------------#\n","        gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(\n","            tonumpy(bbox),\n","            anchor,\n","            img_size)\n","        gt_rpn_label = totensor(gt_rpn_label).long()\n","        gt_rpn_loc = totensor(gt_rpn_loc)\n","        rpn_loc_loss = _fast_rcnn_loc_loss(\n","            rpn_loc,\n","            gt_rpn_loc,\n","            gt_rpn_label.data,\n","            self.rpn_sigma)\n","\n","        # NOTE: default value of ignore_index is -100 ...\n","        rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_label.cuda(), ignore_index=-1)\n","\n","        # ------------------ ROI losses (fast rcnn loss) -------------------#\n","        n_sample = roi_cls_loc.shape[0]\n","        roi_cls_loc = roi_cls_loc.view(n_sample, -1, 4)\n","        roi_loc = roi_cls_loc[t.arange(0, n_sample).long().cuda(), \\\n","                              totensor(gt_roi_label).long()]\n","        gt_roi_label = totensor(gt_roi_label).long()\n","        gt_roi_loc = totensor(gt_roi_loc)\n","\n","        roi_loc_loss = _fast_rcnn_loc_loss(\n","            roi_loc.contiguous(),\n","            gt_roi_loc,\n","            gt_roi_label.data,\n","            self.roi_sigma)\n","\n","        roi_cls_loss = nn.CrossEntropyLoss()(roi_score, gt_roi_label.cuda())\n","\n","        losses = [rpn_loc_loss, rpn_cls_loss, roi_loc_loss, roi_cls_loss]\n","        losses = losses + [sum(losses)]\n","\n","        return LossTuple(*losses)\n","\n","    def train_step(self, imgs, bboxes, labels, scale):\n","        self.optimizer.zero_grad()\n","        losses = self.forward(imgs, bboxes, labels, scale)\n","        losses.total_loss.backward()\n","        self.optimizer.step()\n","        return losses\n","\n","\n","    def save(self, save_optimizer=False, save_path=None, **kwargs):\n","        pass\n","\n","\n","    def load(self, path, load_optimizer=True, parse_opt=False, ):\n","        pass\n","\n","\n","def _smooth_l1_loss(x, t, in_weight, sigma):\n","    sigma2 = sigma ** 2\n","    diff = in_weight * (x - t)\n","    abs_diff = diff.abs()\n","    flag = (abs_diff.data < (1. / sigma2)).float()\n","    y = (flag * (sigma2 / 2.) * (diff ** 2) +\n","         (1 - flag) * (abs_diff - 0.5 / sigma2))\n","    return y.sum()\n","\n","\n","def _fast_rcnn_loc_loss(pred_loc, gt_loc, gt_label, sigma):\n","    in_weight = t.zeros(gt_loc.shape).cuda()\n","    # Localization loss is calculated only for positive rois.\n","    # NOTE:  unlike origin implementation, \n","    # we don't need inside_weight and outside_weight, they can calculate by gt_label\n","    in_weight[(gt_label > 0).view(-1, 1).expand_as(in_weight).cuda()] = 1\n","    loc_loss = _smooth_l1_loss(pred_loc, gt_loc, in_weight.detach(), sigma)\n","    # Normalize by total number of negtive and positive rois.\n","    loc_loss /= ((gt_label >= 0).sum().float()) # ignore gt_label==-1 for rpn_loss\n","    return loc_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"quEPH82HVl--","colab_type":"text"},"source":["## Test Trainer"]},{"cell_type":"code","metadata":{"id":"vRDcPdzJVoAW","colab_type":"code","cellView":"code","outputId":"7c12b499-e400-42b6-f3c2-f40ab6db1947","executionInfo":{"status":"ok","timestamp":1559020914304,"user_tz":-480,"elapsed":15475,"user":{"displayName":"张耿","photoUrl":"","userId":"17787706570393703878"}},"colab":{"base_uri":"https://localhost:8080/","height":737}},"source":["faster_rcnn = FasterRCNNVGG16()\n","trainer = FasterRCNNTrainer(faster_rcnn).cuda()\n","\n","with torch.no_grad():\n","    for n, v in trainer.named_parameters():\n","       print(n, \": \", v.requires_grad)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["faster_rcnn.extractor.0.weight :  True\n","faster_rcnn.extractor.0.bias :  True\n","faster_rcnn.extractor.2.weight :  True\n","faster_rcnn.extractor.2.bias :  True\n","faster_rcnn.extractor.5.weight :  True\n","faster_rcnn.extractor.5.bias :  True\n","faster_rcnn.extractor.7.weight :  True\n","faster_rcnn.extractor.7.bias :  True\n","faster_rcnn.extractor.10.weight :  True\n","faster_rcnn.extractor.10.bias :  True\n","faster_rcnn.extractor.12.weight :  True\n","faster_rcnn.extractor.12.bias :  True\n","faster_rcnn.extractor.14.weight :  True\n","faster_rcnn.extractor.14.bias :  True\n","faster_rcnn.extractor.17.weight :  True\n","faster_rcnn.extractor.17.bias :  True\n","faster_rcnn.extractor.19.weight :  True\n","faster_rcnn.extractor.19.bias :  True\n","faster_rcnn.extractor.21.weight :  True\n","faster_rcnn.extractor.21.bias :  True\n","faster_rcnn.extractor.24.weight :  True\n","faster_rcnn.extractor.24.bias :  True\n","faster_rcnn.extractor.26.weight :  True\n","faster_rcnn.extractor.26.bias :  True\n","faster_rcnn.extractor.28.weight :  True\n","faster_rcnn.extractor.28.bias :  True\n","faster_rcnn.rpn.conv1.weight :  True\n","faster_rcnn.rpn.conv1.bias :  True\n","faster_rcnn.rpn.score.weight :  True\n","faster_rcnn.rpn.score.bias :  True\n","faster_rcnn.rpn.loc.weight :  True\n","faster_rcnn.rpn.loc.bias :  True\n","faster_rcnn.head.classifier.0.weight :  True\n","faster_rcnn.head.classifier.0.bias :  True\n","faster_rcnn.head.classifier.2.weight :  True\n","faster_rcnn.head.classifier.2.bias :  True\n","faster_rcnn.head.cls_loc.weight :  True\n","faster_rcnn.head.cls_loc.bias :  True\n","faster_rcnn.head.score.weight :  True\n","faster_rcnn.head.score.bias :  True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OEDfian4Yzyx","colab_type":"text"},"source":["## vis_tool"]},{"cell_type":"code","metadata":{"id":"-HlG2cJ-Y1_B","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib\n","import torch as t\n","\n","#matplotlib.use('Agg')\n","from matplotlib import pyplot as plot\n","\n","\n","# VOC_BBOX_LABEL_NAMES = (\n","#     'fly',\n","#     'bike',\n","#     'bird',\n","#     'boat',\n","#     'pin',\n","#     'bus',\n","#     'c',\n","#     'cat',\n","#     'chair',\n","#     'cow',\n","#     'table',\n","#     'dog',\n","#     'horse',\n","#     'moto',\n","#     'p',\n","#     'plant',\n","#     'shep',\n","#     'sofa',\n","#     'train',\n","#     'tv',\n","# )\n","\n","\n","def vis_image(img, ax=None):\n","    if ax is None:\n","        fig = plot.figure()\n","        ax = fig.add_subplot(1, 1, 1)\n","    # CHW -> HWC\n","    img = img.transpose((1, 2, 0))\n","    ax.imshow(img.astype(np.uint8))\n","    return ax\n","\n","\n","def vis_bbox(img, bbox, label=None, score=None, ax=None):\n","    label_names = list(VOC_BBOX_LABEL_NAMES) + ['bg']\n","    # add for index `-1`\n","    if label is not None and not len(bbox) == len(label):\n","        raise ValueError('The length of label must be same as that of bbox')\n","    if score is not None and not len(bbox) == len(score):\n","        raise ValueError('The length of score must be same as that of bbox')\n","\n","    # Returns newly instantiated matplotlib.axes.Axes object if ax is None\n","    ax = vis_image(img, ax=ax)\n","\n","    # If there is no bounding box to display, visualize the image and exit.\n","    if len(bbox) == 0:\n","        return ax\n","\n","    for i, bb in enumerate(bbox):\n","        xy = (bb[1], bb[0])\n","        height = bb[2] - bb[0]\n","        width = bb[3] - bb[1]\n","        ax.add_patch(plot.Rectangle(\n","            xy, width, height, fill=False, edgecolor='red', linewidth=2))\n","\n","        caption = list()\n","\n","        if label is not None and label_names is not None:\n","            lb = label[i]\n","            if not (-1 <= lb < len(label_names)):  # modfy here to add backgroud\n","                raise ValueError('No corresponding name is given')\n","            caption.append(label_names[lb])\n","        if score is not None:\n","            sc = score[i]\n","            caption.append('{:.2f}'.format(sc))\n","\n","        if len(caption) > 0:\n","            ax.text(bb[1], bb[0],\n","                    ': '.join(caption),\n","                    style='italic',\n","                    bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 0})\n","    return ax"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ptbgk_0IMrbg","colab_type":"text"},"source":["## Sanity check"]},{"cell_type":"code","metadata":{"id":"5MUnbFdPMtnh","colab_type":"code","outputId":"8c083e7b-c155-496d-f3e3-51e8ccc9974b","executionInfo":{"status":"ok","timestamp":1559021237179,"user_tz":-480,"elapsed":338317,"user":{"displayName":"张耿","photoUrl":"","userId":"17787706570393703878"}},"colab":{"base_uri":"https://localhost:8080/","height":12917,"output_embedded_package_id":"1QfP36V_DYwWDpkA4KQONH4ujvRMhl-xF"}},"source":["import numpy as np\n","import torch\n","from torch.utils.data import Subset, DataLoader\n","\n","epochs = 20\n","shuffle_mask = np.arange(len(dataset))\n","np.random.shuffle(shuffle_mask)\n","trainset = Subset(dataset, shuffle_mask[:30])\n","train_loader = DataLoader(trainset, batch_size=1, shuffle=True, num_workers=4)\n","\n","torch.cuda.empty_cache()\n","model = FasterRCNNVGG16()\n","trainer = FasterRCNNTrainer(model).cuda()\n","trainer.train()\n","\n","for epoch in range(epochs):\n","    train_loss = 0.0\n","    for it, (img, bbox_, label_, scale) in enumerate(train_loader):\n","        img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n","        scale = scalar(scale)\n","        losses = trainer.train_step(img, bbox, label, scale)\n","        \n","        train_loss += losses.total_loss.detach().cpu().numpy()\n","        if (epoch+1) % 5 ==0 and (it+1) % 5 == 0:\n","            print(\"=====epoch: %d evaluation====\"%epoch)\n","            ori_img = inverse_normalize(tonumpy(img[0]))\n","            gt_bbox, gt_label = tonumpy(bbox[0]), tonumpy(label[0])\n","            vis_bbox(ori_img, gt_bbox, gt_label)\n","            plot.show()\n","            \n","            bboxes, labels, scores = trainer.faster_rcnn.predict([ori_img], visualize=True)\n","            bboxes = tonumpy(bboxes[0])\n","            labels = tonumpy(labels[0])\n","            scores = tonumpy(scores[0])\n","            vis_bbox(ori_img, bboxes, labels, scores)\n","            plot.show() \n","            \n","    train_loss /= len(trainset)\n","    print(\"epoch %d / %d: total loss %.4f\"\n","          %(epoch, epochs-1, train_loss))\n","    "],"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"GzojrUzmZfKM","colab_type":"text"},"source":["## SVHN Train"]},{"cell_type":"code","metadata":{"id":"aOQWbMlxZg5Z","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kOoE4QtvZsXr","colab_type":"text"},"source":["# TODO:  \n","1. SVHN数据集修改\n","2. evaluation metric: map or eval loss\n","3. 银行卡数据修改和应用\n","4. 模型load和save，改成.py文件在pc直接运行"]},{"cell_type":"markdown","metadata":{"id":"JVrYOAKJRbBI","colab_type":"text"},"source":["# Note(riva):\n","\n","\n","\n","\n","1.   VGG16未使用\n","2.   模型save路径\n","3.   数据集对齐？！---银行卡数据(label、boxes)\n","4.   验证过程？\n","5.   可视化---模型连接并使用\n","6.   参数调整-how\n","\n","\n","\n","\n","\n"]}]}